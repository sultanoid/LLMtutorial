hello all my name is krishnayak and welcome to my youtube channel so guys here is one amazing one short video on langchain in order to learn generative AI so if you are interested in creating amazing LLM application or gen AI powered application then this specific video is definitely for you if you don't know about langchain it is a complete framework that will actually help you to create Q&A chatbots, rag application and many more the most interesting thing will be that I will be covering all the paid LLM models along with that open source LLM models even though that they are hosted in hugging phase so we will be getting to know each and everything about it and how we can specifically use in langchain so I hope you enjoy this particular video and please make sure that you watch this video till the end so thank you let's go ahead and enjoy the series before I go ahead guys since you know there is some there should be some motivation for me also so I will keep the like target to thousand please make sure that you hit like share with all your friends and we'll also keep a target of 200 comments okay 200 comments I know you will be able to do it so let's keep that specific target and let's understand what all things we are going to learn about langchain and then we'll also understand the second topic that we are going to understand in this specific video is about the langchain ecosystem and how it is related to each other now right now in the langchain documentation if you probably see the recent updates that are there mainly most of the modules revolve around this particular topics in langchain okay so over here you will be able to see langsmith here you will be able to see langsir if I talk about langsmith recently I had also made a video on this just a simple video but I will try to create more videos on that when we go in this specific series so langsmith is if I probably give you some examples it is help you to monitor your application it will help you to debug your application so in short whatever MLOps activities is specifically required with respect to monitoring deploying as already debugging testing you know so third point I will say testing you can specifically use this amazing module in langchain that is called as langsmith the best thing will be that all the reports all the analytics you will be able to see very much easily in this ecosystem itself in the langchain ecosystem so there is a dashboard in langchain which you will be able to see it okay now how we are going to use in this use this entire technique in some projects we will be seeing completely end-to-end and will also be able to understand this so if I talk about langsmith that is mostly of if I say LLM ops okay so this part that is right now required in many many companies so that part will also be able to cover it that is the reason why I like langchain because it is providing you the entire ecosystem irrespective of any LLM model okay any LLM model now coming to the second thing over here you have Langsir let's say that you have created your LLM application you obviously want your entire LLM application in the form of APIs right without writing much code yes you can write the code from scratch with the help of flask or some other some other libraries but here Langsir uses something called as fast API okay and because of this fast API you know creation of this particular API's becomes very much easy so we will be also able to understand it before the deployment if I have actually created my own LLM app how I can actually create all the services in the form of APIs that will try to see with respect to the Langsir now coming to the next thing there are some amazing concepts and important concepts in langchain from data ingestion to data transformation and all in that major major topics are with respect to chains will try to understand about chains and probably in the next video once I probably start the practical implementation the first thing that I'm actually going to cover is with respect to chains and I'm also going to discuss something called as agents and retrieval right not only that you will be able to see there is a concept of LCEL okay now in this LCEL we will be discussing and this full form is the langchain expression language right so there are a lot of concepts that are specifically used in LCEL we will also see that how it is basically important while you are building what are techniques it actually has when you're creating your own generative AI powered application right along with that there are three main topics also which I will be able to cover it while I will be discussing all these things that is about model IO retriever agent tooling and all these are concepts that you should really know the main aim of this entire series is not to make you understand in theoretical concept but it's more to understand how you can create amazing generative AI applications irrespective of any LLM model now see guys one of the question that I get from many people hey Krish I'm using this specific LLM model I don't have opening access I don't have API access tell me what should I do you know let's say many people say that hey Krish I don't have credit card for open AI right can you show me some examples with respect to open source models like Google Gemini or some other Mistral what about open source models right see LLM over here as I said langchain is an amazing framework to build the entire application and creating the best LLM is already a rat race so if I probably say this is a rat race by tech giants right so Google will be competing meta will be competing you know anthropic will be competing opening I will be competing so you don't even have to worry about this right whatever models will be probably coming up in the future don't worry about this the main thing is that how you can use this LLM app in a generic way to build any kind of application so here whatever model it may come you just need to stay up to date right which model has the best accuracy and all that will basically happen over here right later on all the integration part will be completely generic okay now let's go ahead and understand the entire langchain ecosystem like what are the very important things so this is the diagram that I have already taken from the langchain so as I already discussed with respect to langsmith it's the first model that you see over here we basically say it as observable observability right and with the help of langsmith you will be able to do debugging playground evaluation annotation and monitoring when I say annotation it's all about creating your own custom data set which will require with respect to fine-tuning on creating your gen AI powered application the next thing is with respect to deployment and recently right now Langserv has actually come it will be in the form of API so here it is written right chains as REST API whatever services you are specifically providing langchain soon will also come up with only one click deployment mechanism once you probably create this entire API is with the help of Langserv the next thing is that you just need to deploy it okay then and already I have created a couple of videos on Langsmith and Langserv but don't worry in this entire series I'm going to again start from fresh combine new topics and create a project okay the third thing that you really need to understand is about templates okay so they are different different templates reference the application with respect to langchain you need to understand is three important things change agent and retrieval strategies so we'll try to understand that how these things work and understand guys you will be able to get the code with respect to Python and JavaScript but my main aim will be with respect to Python because in some of the article it was already said that AGI application is going to get created in Python okay then you also have some integration components so we are understanding about the ecosystem because all these things we are going to discuss in this specific playlist itself so here you have model IO here you have retrieval here you have agenting tool so retrieval you also will be having features to read your data set from different different data sources how you can actually create vector embeddings and all and all are there right in model IO you have various techniques with respect to model chain prompt example selector and output parser and finally you will be also seeing that we'll be focusing on this amazing thing which is called as protocol which is basically coming under langchain core and here we are going to discuss about langchain expression language right so there are some important concepts like parallelization fallbacks tracing batching streaming async and composition so all these things we are basically going to cover and then we will be focusing on end-to-end projects how you can use all these concepts together right understand one thing guys when we create lab when we use Langserv and start creating REST APIs we'll also be writing our we'll also be writing our client-side code so that we'll be able to access those kind of APIs right so all these things in the form of ecosystem will get completely covered trust me again why I'm saying this is this is important because tomorrow whatever LLM models may come right how advanced it may be langchain will be a generic framework which will actually help you to build any kind of LLM application in this video I will be showing you how we can create chatbot applications with the help of both paid APIs LLM along with that we'll also see how you can integrate with open source LLMs now you should definitely know both this specific ways how you can actually do it one way to basically integrate any open source LLM is through hugging face but as you know that I'm focusing more on the langchain ecosystem and with respect to hugging face I've already uploaded a lot of videos in my youtube channel and you can actually call this kind of open source LLMs but since we are working with the langchain ecosystem we will try to use all the components that are available in langchain as you all know guys this is a fresh playlist and obviously my plan is that this month I will be focusing entirely on langchain many more videos will be coming up many more amazing videos along with end-to-end application
fine tuning many more things is going to come up so please make sure that we'll keep a like target for every video and for this video the like target is 1000 and at least 200 comments and please make sure that you watch this video till the end because it is going to be completely practical oriented okay and if you really want to support please make sure that you subscribe the channel and take up membership plan from my youtube channel so that it will help me and with the help of those benefits I will be able to create more videos as such so let me quickly go ahead and share my screen so here is my screen over here and you'll be able to see in the github that you'll be finding in the description of this particular video you'll be having folders like this so today is the third tutorial not third second tutorial in the first and second we just understood that what all things we are going to learn but in this is the real practical implementation that is probably there so as usual the first thing that we are going to do is that create our venv environment how to create it conduct create minus p venv python is equal to 3.10 you can probably take 3.10 version and I have already shown you how to create virtual environments in many number of videos then you'll be using .env file so this will basically be my environment variable in this environment variable I will be putting three important information one is lang chain API key and the second one is open AI API key and lang chain project you may be thinking this open AI API key I have kept it as open no it is not I have changed some of the numbers over here so don't try it out it will be of no use okay and then the third environment variable that I'm actually going to create is my lang chain project name that is tutorial 1 I have written it over here the reason I have written this because whenever I try to go ahead and see in my lang smith right I will be able to see observe the entire I'll be able to monitor each and every calls from the dashboard itself how we will be using this everything I will be discussing about it okay so all these things will specifically get required and all this will be used in our environment variable so these are the three parameters I have already created my .env file so let's go ahead and start the coding okay and you have to make sure that you code along with me because this is the future AI engineering things are basically coming up easy I'll just show you initially with the foundation model later on this complexity will keep on increasing so let's go ahead and start our first code now what is our main aim what we are trying to do in our first project let me just discuss about because these are all the things that we're going to discuss in the future but first thing that we will try to create is our normal chatgpt application okay I'll not say chatgpt but a normal chatbot okay and this chatbot will be important it will be helping you to probably create chatbot with the help of both paid and open open open source LLM model so this will be the chatbot that we will be creating one way is that we will be using some paid LLMs now paid LLMs one example I can show it with the help of open AI API okay open AI API the second one that I will try to probably show it or you can also use cloudy API so that is from a company called as anthropic okay that you can do and one more I will try to use it with the help of open source LLM see calling APIs is a very easy task okay but the major thing is that since we have so many modules we are going to use lang chain as suggested right and in lang chain we definitely have so many modules how we can use this modules for different different calls and along with this whenever we are developing in the chatbot application what all dependencies we have specifically right dependencies now if you probably see this diagram here you will be able to see there will be model prompt output parcel so in our video in this video I'm going to see some of the features with respect to lang smith I'm going to see some of the features with respect to chains and agents and I'm also going to use some of the feature present in model and output parcel so all this combination we are going to specifically use and that is the reason how this is how we I'm going to create that all the projects that we are doing entire videos that are probably going to come up will be much more practical oriented okay so now let's start our first chatbot application so here I will go ahead and write from lang chain okay from lang chain underscore open AI since I'm going to use open AI import chat open AI okay chat open AI so this is the first one that we're going to basically do from lang chain see this three things will definitely be required then one is chat open AI or whatever open AI your whatever chat model that you are going to use how to call open source I will also be discussing about that first of all we'll start with open AI API itself okay so from lang chain underscore core dot prompts I'm going to import chat prompt template okay chat prompt template so this is the next thing that we are probably going to use chat prompt template okay at any point of time whenever you create a chat bot right this chat prompt template will be super important right here is what you'll you'll basically give the initial prompt template that is actually required okay the third library that I'm actually going to import is from lang chain underscore core dot output underscore parsers okay import str output parser okay now this three are very important this string str output parser is the default output parser whenever your LLM model gives any kind of response you can also create a custom output parser that also I will be showing you in the upcoming videos okay this custom output parser you can do anything with respect to the output that probably comes you want to do a split you want to make it as a capital letter anything right you can write your own custom code with respect to this but by default right now I'm going to use just str output parser now along with this the next thing that I'm actually going to do is that I'm going to use streamlet as st okay streamlet as st then I'm going to also import OS and since I'm also going to use from dot ENV import load underscore dot ENV so that we will be able to import all our libraries okay so let's see whether everything is working fine or not okay from dot ENV so here I'm going to basically write Python load underscore dot sorry Python app dot py I'm just running it so that everything works fine and all our libraries will also get in there cannot Python app dot py okay I have to probably go to my chatbot folder cd chatbot so now I'll clear my screen Python app dot py sorry from streamlet as st okay import streamlet as st you have to write so that is the reason it was coming all these errors now let's see if everything is working fine lang chain core so here you can probably see that there is a spelling mistake okay but I'm just going to keep all the errors like this so that you will be able to see it Python app dot py if everything works fine dot output parser okay P capital so I think my suggestion box is not working well and that is the reason now everything is working fine here you can see that I'm not getting any error so let's start our coding and let's continue it okay so we have imported all these things right now now as I suggested guys since we are going to use three environment variables one is the open AI API key lang chain API key and along with that I will also make sure that the tracing to capture all the monitoring results I will keep this three environment variable one is open AI API key or lang chain tracing version 2 and lang chain API key so lang chain API key will actually help us to know that where the entire monitoring results need to be stored right so that dashboard you'll be able to see all the monitoring results will be over here and tracing we have kept it as true so it is automatically going to do the tracing with respect to any code that I write and this is not just with respect to paid APIs with open source LLM also you will be able to do it now this is the second step that I have actually done now let's go ahead and define my prompt template simple so here I'm going to write my prompt template okay from template so here I'm going to define prompt is equal to chat prompt template dot okay from underscore messages okay and here I'm going to define my prompt template in the form of list the first thing that with respect to my prompt template that I'm going to give is nothing but system and system here I say that you are a helpful assistant please respond to the queries okay please respond to the questions or queries please response to the user queries okay whatever queries that I'm going to specifically ask a simple prompt that you can probably see over here the next statement after this is what
one of the function that i am going to use is create stuffed document chain now what this exactly does this chain takes a list of documents and formats them into a prompt then passes that prompt to an llm see this this chain takes a list of documents and formats them based on the prompt formats them into a prompt sorry not based on the prompt, into the prompt that basically means over here if i go ahead and open my browser here in the context i definitely require my documents based on that context and based on this input i will be able to give the answer so based on this context and based on the input context basically means all the documents that are there available in the vector store inputs are what question i am asking so with the help of this create stuffed document chain what is basically happening this chain takes a list of documents and formats them all into a prompt then passes that prompt to an llm it passes all the documents so that you should make sure that it fits within the context window the llm you are using so what it is exactly doing it will take up all the documents from the vector store it will put inside that particular prompt template and then it will send to the llm and then we finally get the response and that is what we will be using similarly there are different different things also over here like create stuffed document chain is there create sql query chain with respect to sql database for natural language and this is one of the very important project that i will also do in the future one or the other way i will try to use one or the other functionalities to just make you understand how we can use all these functionalities itself right but it is always good that we have a specific use case ok now if i open this ok lets go ahead and create my chain how do i create my chain over here again it is very simple so i will write from lang chain lang chain underscore community so it is present inside community itself or not community sorry chains lang chain underscore chains dot combine documents import create stuffed document chain now how do i know this ok i did not create this i have already seen the documentation ok that is the reason i am writing then i will go ahead and create my document chain now inside this document chain as i said i will be using create stuffed document chain ok that we have already seen now inside this chain two things are basically required one is the llm model and the second one is the prompt that i have created because inside this prompt itself the list of documents will be added right whatever documents is basically coming from here right from my vector store that will be added over here ok so once i create this so this basically becomes my document chain ok very much simple now after this we also have to learn about one very important thing which is called as retrievers ok so i will go ahead and write something called as retrievers retriever lang chain ok now what exactly retriever lang chain is it is an interface that returns document even an unstructured query it is more general than a vector store right a retriever does not need to be able to store the documents only to return or retrieve them vector store can be based as the backbone of the retriever now see there is a vector store which is having some information some vector stored in it right if i want to take out any data from there right i can actually do a similarity search which we have already seen ok but lang chain what it did is that since we usually do a lot of programming in a way right wherein classes are used interfaces are used so it created a separate interface which is called as retriever and that interface has a backend source to that particular vector store to retrieve any information right whenever a query is given right that entire vector store will be passing the information through this retriever ok so what we will do is that here i will quickly open this i have also written some amount of description so that it will be helpful for you whenever you probably go ahead and check it this entire material ok so now what i will do i will just go ahead and write db dot db dot as retriever now once i do like this db dot as retriever this basically has become my retriever right so what we have done db is our vector store already it is there we have connected to an interface which is basically this particular variable now ok so if you go ahead and probably display this what is retriever it is nothing but it is a vector store retriever internally you will be also able to see that what all it is implemented files and open AI embeddings and all the information are there now retriever is done chain is also done ok now is the time that what i will do i will try to use this retriever and document chain both together to probably see when we combine both of them then only will be able to get the response right so with respect to this now let's go ahead and create my retriever chain ok so the next step is what since i need to combine both of them one is retriever and one is the document chain right this document chain is responsible for putting the information in the context when we combine both of them then it becomes a retriever chain now what is the definition this chain takes an input as a user inquiry which is then passed to the retriever to fetch the relevant documents so it passes through the retriever it is connected to the vector store then those documents are then passed to an LLM to generate the response and this LLM that we are basically getting it is basically coming from what this document chain understand the flow ok so let me just go ahead and mention this flow again so that it becomes very much easy for you so whenever the users this is my user ok whenever the user ask for any inquiry ok any inquiry so first what it is going it is going to this retriever ok very much important ok so this is my retriever this retriever is an interface to what vector store which has all the information right so once we basically get the retriever then the next step what it happens it goes to what it goes to my LLM model with some prompt right there will be some prompt involved to this and how this is basically happening with the help of stuff document chain so this stuff document chain has already both this things combined LLM and prompt right and then finally we get our response I hope you are able to understand this right and this is what we have basically implemented over here now how to basically create a retriever chain so first for all of this again I will be using a library with respect to chains ok so form lang chain lang chain dot chains import create retrieval chain right and there are lot of things create source chain retrieval chain this chain that chain I will try to explain you all of this don't worry ok I will catch the right kind of use case and I will be showcasing you all this things don't worry about that ok then what I will do I will take this entire create retrieval chain and then create my retrieval chain so here I will write retrieval chain is equal to create retrieval chain and here I am going to use first parameter that I am going to use is retriever then the second parameter is nothing but document chain so once I have this chain right now I will be able to invoke any queries so I will go ahead and write retrieval chain dot invoke ok and now what are the parameters that I have to give nothing I have to give my input so the input will be given over here colon and whatever input that I can give let's say from the pdf I have put some input over here let me just copy and paste it ok so this is one of the text that is available in the pdf that is attention dot pdf and now if I invoke this you will be able to see that I will be able to get the entire response ok so retrieval dot chain dot invoke and again we are using open source LLM model that is llama2 ok yeah I have used OpenAI Embedding so here you can see this is my input this was the context right all the context information is there and finally I get the answer and this answer I will just try to save it over here something like response response ok and then I will execute response response of answer so this finally becomes my output that is probably coming over here ok so this if I execute it the answer to the question is right and all the information you can see the answer to the question and it is retrieving all the details right so here you will be able to see that how beautifully with the help of LLM we have constructed this entire thing and we have used this chains retriever and this is the first step towards developing your advanced rack pipeline right so whatever question you ask let me
just open this and probably show you some more examples. Okay. So what I will do, I will just open my download page. Let's see my download page. I'll ask for any statement. Okay, just a second. Attention. Okay, so this is my thing. Let me just go ahead and search for anything. The decoder is the this and this. Okay, so I'll be searching from here to here. Okay. Now, let me go ahead and change my input and search for it. So chain dot invoke I've done. And here I've got the response. Now let me just go and I think the answer to this question is six. The decoder is also composed of stack of n six. Oh, it has basically taken this. No worries. Okay, let's take some more thing. I'll write scaled dot product attention. Some more examples. Okay, just a second. Now I think it should not be. Okay, it is taking that question and it is trying to form some answers out of it. Okay. Got it. Got it. It is not a mistake. I thought it was a mistake out there. scaled dot product is a type of this this this see, I'm getting all the answers over here. So we are going to continue the Lanshan series. And in this video, we are going to probably develop an amazing multi search agent rag application. What exactly is the project all about what all new things we are going to specifically learn and probably I think this is the most amazing thing that has been inserted in the Lanshan library itself, some of the amazing modules that are there, which will actually help you to make your entire conversational chatbot quite amazing itself, right. So let me go ahead and discuss about this what what exactly we are going to do what kind of projects we are going to implement here. So let's consider that I want to generate a gen AI powered LLM application. So this is my LLM application that I really want to generate. And this let's consider that it has dependencies on some of the other open source platforms or the websites or the data sources itself, right, like like our safe, our safe, if you don't know, like all of the research papers that are available will be available over here. If I probably consider with respect to Wikipedia, right, Wikipedia also has a huge amount of content. Similarly, I may also have let's say my own company's PDF, you know, from where I also need to probably develop a q&a applications, right. So all these things, we will probably try to implement it. Now, the main thing is that here you have multiple data sources, and you really want to integrate all them as a wrapper so that you will be able to implement this entire q&a solution, right over here. So here, you're also going to learn about some new terms. And right now, it is becoming very popular like tools, right tools, what are tools in lang chain, what are agents in land chain, what are toolkits, we'll also be learning about toolkits, right? And how do you probably create a wrapper on top of this particular toolkits. But if I just want to give a small brief introduction about tools, you can see that here, I have dependency on this platform on this platform, I can probably use all these platforms as a separate tool, so that I will be able to ask any questions from this particular platform itself. Along with that, I may also have my own customized PDF own custom data, which will also be in the form of vector embedding. So what I will do is that I will try to wrap this up in the form of toolkit. And then with the help of agents, I will be able to execute any q&a search that I really want to do it. Yes, I have probably told a lot of terms and topics over here. But let's go ahead and let's see how we can actually implement it. And all this implementation that you will probably be seeing will be quite amazing. A lot of learnings will be specifically there. And you will definitely be able to learn many things out of it. Okay, so let me quickly go ahead and open my code file. And I will continue the same code file that I had actually implemented earlier itself. Right. One more very important thing that I really want to talk about is that if you remember, I have uploaded five videos till now. And all those videos are with respect to some different different applications, right. And that is how we should definitely learn all these topics so that we can actually get an efficient way of how things are basically getting used. Okay, so as usual, what I'm actually going to do is that let's me go ahead and search for Lang chain tools. Okay. So here, if you if you're able to see here, you have something called as Lang chain tools. So tools and interfaces that an agent chain or LLM can interact with the world, right. So if I really want data from some other data sources, some other queries that I have, you can also include Google search API over here. A lot of tools are specifically given by Lang chain itself, you know, we have to really create a wrapper and then we will be able to have a conversation with them. So some of the examples with respect to the tool, like what all tools are specifically provided, you will be able to see in Lang chain, these are all the built in tools that you have alpha advantage, FFI, RC, AWS, lambda, Bing search, brave search, chat, GPT plugins, you know, daily image generator, you can also be able to generate images, then you have Eden AI file system, golden query, Google finance, Google jobs, Google lens, Google places, Google squalor, Google search, Google server. So you can probably use any of these particular tools to extract some of the data that you want. Let's say that I am planning to create a chatbot, which will be research more oriented. And I will probably ask questions that will be related to different topics, research and all. Other than that, I will also try to create a wrap on top of Wikipedia. And along with that, my under the customized PDF files, okay. So I will try to include all those things. And you can use any of this that you want, based on your use cases, but I will try to show you an application how I will go ahead with Okay, so let me quickly open a new folder over here. So I will go ahead and create a folder. And let me go ahead and write agents over here as my folder name. And I am going to create a file which is called as agents dot IP y nb file. Okay. Now inside this particular file, I will go ahead and select my kernel step by step, I will try to show it to you how things will be working in this and how I'm going to build this entire thing. Okay. So the first thing as usual, as I said that I will be requiring our safe, so you have to probably go ahead and install this. So I will go ahead and write pip install our safe. So okay, double s, no worries. So I will go ahead and install it, it will go it will get installed in my VNV environment. Once this is basically getting installed, what I am going to do is that I'm going to use Wikipedia, I'm going to use our safe also, and I'll create as a wrapper. Okay, so first of all, let's go ahead and create a wrapper on top of Wikipedia. So for that, what I will do, I will go ahead and write from lang chain, lang chain, underscore community. So it will be present inside this community itself, all the tools that are available within the lang chain itself, then I'm going to import Wikipedia, Wikipedia, query run, okay. So I'm going to just write Wikipedia query run, I hope this works. Let's see whether this works or not. Okay, this is my lang chain underscore community. But I think it should be working now. From lang chain dot community dot tools, import Wikipedia, query run along with this, what I'm going to write from lang chain, I'm also going to create a use some wrapper class on top of it dot utilities, utilities, import Wikipedia API wrapper, okay. So I'm going to specifically use this two things over here. One is the Wikipedia query run and Wikipedia API wrapper. Okay, for Wikipedia don't require a separate API itself already lang chain is taking care of that. So if I go ahead and write and probably execute this Wikipedia API wrapper, I will say hey, provide me the top k results, right? So how many results I want, let's say I want one. So I'll go ahead and write document content char max, max to max, I will probably require 200 characters, okay, from whatever search I actually do from Wikipedia, you can increase it. So this will basically be my wrapper itself API wrapper. Okay, so this in short is interacting with the Wikipedia to find out the so many number of results. So this is basically giving some configuration details over here. The next will be my tool. As I said, I will be using my Wikipedia query run tool, okay. And then this will be initialized. And here, I'm going to initialize my API wrapper with the API wrapper that I've defined over here. Okay, so this is what is my tool over here. So I'm getting some error could not import Wikipedia Python package, please install it with pip install Wikipedia. So I will go over here and requirement txt, I will go ahead and write Wikipedia, okay, I will be requiring this library. Along with this, as I said, I'm going to import also our shift. Okay. So let me quickly go ahead and open my terminal and I will start the installation over here. Okay, so let me go ahead and write pip install minus our requirement or txt. So both these libraries will get installed that I specifically want initially you actually require it, whatever is there, these packages are available and Langston is just trying to integrate with that. And that is the most amazing thing over here. Okay, so now I don't think so we should be getting an error. So now it looks good. Now if you go and write tools over here or tool, tool name over here. So here you can see Wikipedia query run, if I go ahead and write tool dot name, you'll be able to see Wikipedia is my tool name. Okay. Now, this is one of the tool. Similarly, what I will probably do is that I will also take a website or a PDF, whatever it is, right.
will read all those kind of PDF okay so that I will also consider that as an data source so let me do one thing let me I will probably create another tool over here okay so let's see over here I will go ahead and go to this website okay so let me just see this website I've just copied this okay so this website this website okay so this is the website that I will also be using and I'll try to retrieve the content from here also so in order to read this content from this particular website I will be using web based loader as you all know we have also discussed about that because that is also one of the data ingestion thing right so I will write from lang chain underscore community dot document loaders and here I'm going to specifically import web based web based loader okay along with this as you all know since I'm reading the content I will be using files I'll be using open AI embeddings and all so let me copy it from here and let me paste it over here okay so I'll be using files I'll be using open AI embeddings I'll be using recursive character text splitter so that I will be able to divide all those into chunks also so everything will be used over here in this particular case and one thing that you really need to understand why we are doing this because this is my own custom data let's consider this as my own custom data I need to probably convert this into a vectors so here I will go ahead and load my loader and here I will be using web based loader and I will give my URL the URL that I had actually got from here so this URL will be basically reading the entire page okay once I probably get the loader dot load okay loader dot load it will load the entire content from that particular website so this will be my docs okay after getting the docs the next thing that I will do is my recursive character split okay and here I am going to specifically use some chunk size so this will basically be my chunk underscore chunk underscore size is equal to thousand and then I will also be writing chunk underscore overlap which will be nothing but 200 okay so this is some default configuration I have actually taken in this and then I will also write dot split underscore documents I will split all these documents that are available inside this docs okay and I will finally get my documents itself okay and this we have already done it many number of times I think in my previous tutorial also then I will go ahead and create my DB my vector DB so I will go ahead and write vector DB vector DB and here I am going to specifically write files files dot from documents from documents and here I am going to give my documents comma my opening I am meetings that I am specifically going to use for this particular right and later on if I really want to convert this vector database into retriever all I have to write is vector DB dot as retriever as retriever if you don't know what is retrieved is an interface which will be able to retrieve the result from this particular vector database right so here will be my retriever right so this is my second important thing so let me go ahead and write retriever retriever okay so I have done I've created one tool have created one retriever tool is for Wikipedia but still I have also installed our ship right so I'm also going to use RC for the same purpose first let's all this particular thing happen now you have that Victor the vector store retriever completely over here okay now the next thing that I am probably going to do over here is that I will take this retriever and use create retriever tool so that I can actually make it as a as a as a in short like if I really want to ask any question I really need to make this retriever as a create retriever tool okay so if you go ahead and search for Lang chain for Lang chain create retriever create retriever tool okay so here you will be able to see that this is what we are going to specifically use and create retriever tool you can see create a tool to do retrieval of all the documents so we are going to specifically use this other than this if you can definitely check out all the agents that are probably available over here for that purpose right and by that you will be able to implement things so here and what I'm going to do I'm going to create my retriever tool itself so I will write from Lang chain dot tools dot retriever import create retriever tool I will initialize this create a retriever tool and here I'm going to use retriever is equal to whatever retriever name I have and then this will basically be my lang smith search right so because that is with respect to the lang smith page okay land smiths underscore search okay so that it will be able to get identified okay like which which tool I am basically hitting or which tool I am basically searching okay and here I will give my third parameter and I will say hey search for information about lang smith I'll just copy and paste it I've already done this in one of my project so so here you can see the third parameter that I'm giving in the create retrieval tool is just like a prompt like what I really want this tool to do search for information gain about lang smith for any question about lang smith you must use this tool so in short when I do this create a retrieval tool right it is basically creating a tool to do the search for that particular page right so here I'm going to write retrieval underscore tool okay and this will get initialized right so I have got one tool over here one tool is Wikipedia and the other tool if you go and see this retrieval underscore tool dot name this is nothing but lang smith search now the third retrieval tool that I am actually going to create is specifically my own R shift platform that I have actually developed so for this also I will be having one wrapper and one query run okay so this will basically be for my R shift. R shift basically means the website where all the research papers have been uploaded okay so let's create one more tool so this is for this particular tool okay and again the same thing that we will try to do is that we have to create a wrapper API wrapper R shift wrapper where again my document max search will be 200 and this will be basically my for my query run and here I'm actually going to get my R shift dot name and here you will be able to see that this is my tool right so this is inbuilt tool that is also provided by lang chain Wikipedia and R shift is provided by lang chain if you really want to create your own custom tool then you can also create like this but like I have actually shown you right at the end of the day you combine all these particular tools so let me go ahead and combine it so I will write tools is equal to and let's go ahead and combine this so the first tool that I have I think it is name is tool only okay so I have at written tool so let me go ahead and write wiki wiki over here wiki okay wiki wiki okay so here I'm going to combine all these things so first one is wiki the second one is my R shift and the third one is my lang smith okay so lang smith this is nothing but it is a retrieval tool name right so I'm going to basically combine all these tools so finally I got my entire tools over here so this tools is nothing but it is a list of all the tools that you can see over here with respect to this now my next aim is basically to query from the specific tools right so for this I can do it in multiple ways and that is where we will be specifically using agents okay so let me just go ahead and tell you what is the main purpose of agents agents will be responsible in probably see if I go ahead and show you the documentation also let me go ahead and show you the documentation agents the core idea of the agent is used to land you is to use a language model to choose a sequence of action to take in change the sequence of action is hard-coded in code in agent a language model is used as a reasoning engineering to determine which actions should take place in which order right now here you can see that I have given one two three tool in that specific order so what my agent will basically do is that it will whenever I perform any request right whenever I give any input to my LLM model it is first of all going to first search in wiki if it is not able to get in from this particular tool it is going to go to our ship then it is going to go to retrieval tool right so from these three tools it is going to get the query and it is going to provide you the response okay so let me quickly go ahead and write from line chain dot agents I'm going to import create create open AI tool agent and here I'm going to define my agent name and let me go ahead and write create open AI tools agent name and here I'm going to give my LLM my tools that I have actually created and one will be my prompt okay so the prompt and tool I have not yet defined so what I will do here I will show you an example like how you can probably call your prompts also that is already available in Lang chain hub but before that let me go ahead and call my open AI API so quickly I will go ahead and call it and then this is what I'm initializing my LLM models as okay
So I have actually created chat open API. So this is what I'm actually going to use. Okay. Now, let me go. So this is my LLM model. The next thing is that how can I probably create a prompt. One way that I've already showed you of creating a prompt is just by using chat prompt template. But in Lang chain, there is a module called as hub there people have already created or Lang chain has already created some amazing prompts or generic prompts and they have uploaded over there. So in order to call from there, so I will write from Lang chain from Lang chain, import hub. Okay. And then we are going to prem give the prompt we are going to get the prompt and the prompt will be of this name. Okay, so we will also see see, get the prompt to use and you can modify this. So there is something called as open AI function agents. And this is the username that is present in the Lang chain hub. If I go ahead and see what is the prompt over here, so I'm getting an error. Let's see. Please install Lang chain hub. Okay. So we also require Lang chain hub in order to use this. See the reason why I'm showing you all these things, they've got so many different options are there. Lang chain hub. Okay, so let me quickly go ahead and run in the terminal pip install requirement.txt. Okay, so this I will probably Lang chain, Lang chain underscore hub. Let me see the error. What was the error over there? Okay, it should be something like this. Okay, so I will go ahead and write or see in the requirement.txt. The name is different. Now let me quickly go ahead and write it down away and install it. Right, so here you can probably see, yes, the installation is done, my agents is ready. Now let me go ahead and execute this. Now I think you should be able to see the messages. Okay, now you will be able to see the messages over here. Now see, by default, whatever prompt is available in OpenAI function agents, you'll be able to see I have system message prompt template, there is prompt variable, the input variable is there in template, you are a helpful assistant. So what we used to do manually, everything is available over here. The same thing, it will have one human message, one system message prompt template, one human message prompt template, right? Now, you have the prompt, you have everything. Now we can go ahead and create my OpenAI tool agents. Once I probably create my OpenAI tool agents. Now you can probably use this agent and execute to get any kind of response. Now in order to use this particular agents, we have to use something called as agent executor. Okay, agent executor, please understand the flow, the flow is important. And that is how you will be able to understand this. So from lang chain dot agents, import agent executor. Okay, so agent executor is my next, I'm that I'm going to specifically use and I'll use this agent executor. Here, I'm going to use my agent is equal to agent. Along with that, I will use my tools is equal to tools. Okay, whatever tools I've given, and then this verbose will be true so that I'll be able to see all the details whenever I get any response. And this will basically be my agent executor. So this is basically responsible in executing anything, right? So now if I go and see my agent executor, so here you can see all the things and it has also added runnable binding arguments over here, right? Now in order to execute anything, see, if I write agent executor dot invoke. So once I write invoke, and I give my input. And let me give the message, tell me about lang smith. Now you should just think that from where should this input come? Already in our tools, we have our C platform, we have lang smith search, right? And we have Wikipedia. So obviously, this, this agent will execute and it will interact from the tool that is related to lang smith. So once I execute this, here, you'll be able to see what kind of response I usually get. Okay. So here you can see tell me about lang smith lang smith is a platform for building production grade application, LLM application and here and this is probably coming from the tool itself, right? Now here, mostly the thing that you could not see, right, you could not see over here, what exactly the thing was, the reason was very simple, I did not write the verbose parameter properly. Now you'll be able to see what details you'll be getting. See, once you write verbose is equal to true, you'll get all the details, what it has hit now invoking lang smith search, right? It is basically searching the lang smith search tool, right, whatever tool was there, right? Let me go ahead and execute once again, with respect to some different query, right? And here, let me write, tell me about machine learning. Now, I don't know from where it will go ahead and execute it. But let's see whether it will go with Wikipedia or RCF. Okay. So agent executor enters invoking Wikipedia with machine learning, see, and automatically Wikimedia, it is probably giving you the entire result. Okay. Let me also try with something like RCF platform. So I have one question with some research paper, okay, research paper name. So let me just go ahead and execute what's the paper all about the paper number is there. And in RCF, we'll be able to find out this. See, now it is invoking the RCF with query, this particular tool. And here you will be able to get the response. And that is the reason why I say this agent executor tools are probably the most important thing whenever you probably develop a rag application, guys, we are going to continue the lang chain series. And in this video, we are going to create an amazing end to end rag project with the help of open source LLM models and grok inferencing engine, an amazing project all together. Over here, I'm going to show you many more things. And the best part is that many of you were specifically requesting only for open source LLM models. So here I will try to show you from embedding to using all the open source LLM models along with that what exactly is grok inferencing engine, we'll also discuss about it. So here is the grok inferencing engine. If you don't know about grok, it is on a mission to stack the standard for Gen AI inferencing speed, right. And it specifically uses something called as LPU language processing unit, okay, it is a new type of end to end processing unit system that provides the fastest inference for computationally intensive application with us with a sequential component to them. Why it is so much faster than GPU because it solves to LLM bottleneck one is compute density and memory bandwidth. I've already created a detailed video before and spoken more about what exactly is LPU inferencing engine. But now it's time that we go ahead and implement some amazing solutions out here. Right. So let me go ahead and let me implement this end to end project. And this project here, I'm going to probably use streamlet over here so that you can actually see it in a form of web application. So first of all, as usual, what I will do is that I will go ahead and see in the requirement or txt here, you can see grok is also available. And along with that, we'll also be using Langston grok. So both this is the most important part along with that BS for beautiful souffle. So that will be also able to scrap from a website. So that is what I'm actually going to do in my project. So here you can see in grok, I will create one folder that is called as one file that is called as app dot p y. Now quickly, let me go ahead and start the coding over here. First thing first, I will first of all go ahead and import all the important libraries like import streamlet as st. Along with that, along with streamlet, I'm also going to go ahead and import OS. Okay, now, the first thing that you actually require is the API access. So click on the API access of grok, and you will be able to access it completely for free. So once you go to this API access page, you'll be able to see that there is a something called as self serve through playground on grok. So grok cloud also gives you a playground along with this if you really want to use it for a commercial purpose, then you can see currently llama 270 B that is 70 billion parameter gives you 4096 context length. And the response that you will be probably getting is 300 tokens per second, this is quite fast, when compared to open AI, chat GPT specifically, and the price per 1 million token is somewhere around 1.70 point $80. Similarly, llama 270, you can see 750 tokens per second, then mistral is there for 80 tokens per second gamma 70, which is basically having 8k context and this is somewhere around 32k context length, right 820 tokens per second, and the price is also very much minimal. So as usual, see, you can see grok has also demonstrated 15 times faster LLM inferencing performance on an artificial intelligence dot AI leaderboard compared to the top cloud based providers. So this is awesome. And you should definitely know this. So in order to create the API, I will go ahead and click on the playground on grok. Once you go over here, you're here, you'll be able to see the button of API. So just go ahead and click on Create API, give the API name, and just click on Submit. Once you do that, you will be getting an API name itself, right. And this API, this secret key only I will be specifically using in my environment variable. So I'll show you step by step already, I have created those kind of API key. So please make sure that you also update your environment variable, I'll tell you what name you really need to update. Right. Now here, we have done this. Now since we are going to use this grok with chat grok itself, and that is already present in lang chain, right. So here, what I'm actually going to do, I'm going to write lang chain underscore, sorry, underscore grok import chat grok. Okay, chat grok. So this is the
chat. Oh, here, I have made a spelling mistake line chain, line chain underscore grok, okay, import chat grok. So chat grok is like how we use chat open API. Similarly, chat grok is also there for this purpose. As you all know that I'm going to probably take a website and read all my content from that specific website itself, right. So here, what I'm going to do is then again, from line chain underscore community, I'm going to use web based loader again, from line chain underscore community, then I'm going to use document loaders. As you know, this is the first data ingestion technique that we can actually use import web, web base, web base, document underscore loaders, web based loader, web based loader, okay, so I will specifically use this particular line chain underscore community dot document underscore loaders import web based loader along with this, what I'm actually going to do, as I said, I'm going to only use open source models. So first of all, let's go ahead and do this. And I'm going to use line chain. And one of the embedding technique that I'm specifically going to use is all my embedding because it is completely for free, it is an open source itself. So line chain underscore community dot embeddings, import all our embeddings. So here, you don't even have to worry about anything about opening eyes, right. So all my embeddings, I'm going to use it over here. Along with this, what we can do is that since we are going to read the entire document and convert that into smaller chunks, so for that, I will be using recursive character splitter. So again, from line chain, under dot, line chain dot, again, I'm seeing the documentation guys. So I need to know where this library is actually present the recursive character text splitter. So from line chain, I will be using dot text splitters, toss text splitter, and here I'm going to import a recursive character text splitter. Okay, so this is another library that I'm going to specifically use for this purpose. And as you know that I've already shown you how to use create stuff document chain. So for that also, I will be using from line chain. From line chain, dot chains, dot combine documents, import, create stuff document chain. Okay, so this is another library that I'm actually going to use. Along with this, I'll create my own custom chat prompt template. So again, from line chain, dot, sorry, underscore core dot prompts, I'm going to import okay, line chain, underscore core dot import our chat prompt template. Okay, so chat prompt template, I'm going to use along with this. And as you know, after creating the document chain, I also have to create a retrieval chain. So from line chain, dot chains, come on, you're from line chain dot chains. Okay, sometimes all the time when I'm writing language in there is a spelling mistake. Come on. Okay, import, create, create, create, create, retrieval, underscore rt, r, r, e, t, r, i, v, l, underscore chain. Okay. So yes, one more libraries with respect to this, I'm going to use all the specific things for my purpose. And as you all know that I'm also going to use one more thing so that I'll be able to load all the environment variables. So from dot ENV, import, load underscore dot ENV, I'm also going to import OS I've imported OS. So load underscore dot ENV, I'll just load this so that all my environment variables will load it. And one of the environment variable that I'm going to specifically use is grok API. Okay, so let me load the grok API key, okay, so that whatever I'm specifically running, I'll be able to use the open source LLM models from this specific inferencing engine. So here, let me go ahead and write grok underscore API underscore key is equal to OS dot environment. Okay. And here, I'm going to use grok underscore API underscore key, that basically means whatever grok API key I've taken, I've written in my environment variable with this key is equal to whatever my API key I've got from that particular website. Okay, so you also have to really do the same thing. Okay, so whatever key you actually get, you have to take this in the name, and you have to probably go into environment variables. So environment variables somewhere here and just assign this with that whatever key name because I have created a lot of key over there. And since we are continuing the Langston series, I'm going to probably do in this specific project itself. Okay, so once this is done, here, one more thing I'm going to specifically use, in streamlet, I'm going to use something called a session state. So all those things I'll also be doing with respect to this, okay. And that will also help you to understand. So here, I'm going to write if vector not in, okay, st dot session underscore state, I'm going to create some session states so that you'll be able to understand. So first thing that I'll be creating with the session state is nothing but my Allama embedding. And probably, you'll also get an idea in this video about many things, session states and streamlet and all okay. So first of all, I'm going to create my embeddings, this embeddings, I'm going to assign to my Allama embeddings. Okay, so this is the first parameter that I'll be using, then I'll be using session loader. So session underscore state, okay. And along with this dot loader, so I will load some information from my website. Now what website I'm actually going to consider, let's see, initially, I've used some kind of websites over here. At least somewhere I've used some websites, let's pick up any of the website over there. Okay, I'll probably take previous examples. This is done agents is the agent, let's see what example I've actually taken. Okay, let's pick up this website. Okay, so I'm going to use web based loader along with this website. Okay, so here, I'm going to write web based loader. And here will be basically my website link. Okay, so I'm actually reading the content from this website. Okay, so let me quickly open it over here for you. So that you'll be also able to see it. Okay, so it's a Docs dot Smith, chain, LAN, Lang chain, some information I want from here. So I will try to pick it up from here itself. Okay, so this becomes my loader. Now, you know, after getting the loader, what we really need to do is that we need to from this particular website, we need to get the entire documents. So I will create another session states, session underscore state, okay, dot docs, and I will say loader dot oops, not loader, it should be this one st dot session state lot loader dot load. Okay, once I load it, again, the same thing, whatever we are specifically now after getting the docs, what we have to do is that we have to probably use this st dot session underscore state. Now you know what we specifically do after that, you know, we get the final documents or chunk documents, right? So I'll write chunk documents because I need to do over here as recursive character splitter, right? So here, I'm going to use recursive character splitter. And here inside this recursive character splitter, I have to probably consider this as my docs. Okay. Oh, sorry, over here, I will probably be assigning my or instead of writing this chunk underscore documents, I will write text splitter. So this will basically be my text splitter. Here, I will be using chunk chunk underscore size is equal to 10,000 or 1000 1000 I'll go and write chunk underscore overlap will be somewhere around 200. Right. So this is done. Good. We have we have our text splitter. Again, that is also saved in the session itself. Now I'm going to basically go ahead and write st dot session, session underscore state and here I'm going to create my final documents, final documents, okay. And this final documents will be used along with this st dot session state dot text underscore splitter. And here we are going to basically use dot split underscore documents. Okay. And we are going to probably take the documents over here documents is nothing but all the documents that is available over here. Along with this, we are going to use the embedding that is nothing but st dot session underscore state dot embeddings, right? embeddings, I think it should be coming up, right? I think it is the same thing st dot state dot embeddings, right? So this basically becomes your final document. Okay, so final documents and here you can probably see all the things are available, right with respect to all the state session states that are basically getting created. Now the next thing that I also need to do after probably getting the form all the documents form, then I have to really convert that into vectors also and store it in a vector store. So from line chain underscore community on inside this community, I'm going to specifically use my vector stores. Okay, so Lang, Lang, Lang, Lang, Lang, Lang, all the time, C will be coming before G. So how many of you are also doing the same mistake like me? Okay, so from this one, I'm going to import fires. Okay, now this fires that I've actually initialized again, I'm going to store it inside my session session underscore state dot name it as vectors is equal to I'll be using fires fires dot from documents. And inside this, as you know that since we are going to use this functionality of from documents, two important things I have to really give one is all my
all my final documents. So here, I will give my first parameter as final documents over here, okay. And the second one that I'm specifically going to give is nothing but my embeddings, right. So this will specifically be my embeddings, right? Done. So this is perfect. Here, you will be able to see that I'm able to see all the documents, everything as such. And this is all in short, my, these vectors will get created. Now, I can probably ask any questions from this particular vectors, okay. But since I really need to create this as an end to end project, so I'm going to use multiple things. One is st.title. Okay. And here, I'm going to basically use chat grok, chat grok demo. Okay, chat grok demo. Now, let me go ahead and create my LLM, my LLM is nothing but chat grok. And inside this, I'm going to specifically take my grok underscore API key, which will be initialized to my grok underscore API key, right? So whatever grok underscore API key, I'm actually getting it. Okay. Along with this, the model that I'm going to specifically use the multiple models as already said over here. So let me check out what all models are there. So you have this gamma seven be it right? Or you have this mistral 8732768. So I will use gamma seven be it. So let me quickly go ahead and over here and I'll let me write in my name, model underscore name is equal to. So gamma, I think it is capital letter, gamma seven be. So this will be gamma seven be it, I think spelling is right, right? It's capital I t. Is it capital I t or one t? Okay, if it does not work, I'll change the model name. No worries, I have three other options. Capital it. Okay. I think it should, it should be capital it. Okay. So once this is done, my grok, my model name, everything is basically created, then this is my LLM models. Now what I will do is that I will also go ahead and create my prompts. So let me go ahead and write prompt, chat from template dot from template. So there are also some templates that is defined, you can also create a chat from template from scratch. Okay. So I will say, answer the question. So here, I'm going to basically use some context also answer the answer the questions based on the provided context, context only. Okay. So here, I'll write please provide provide the most accurate response based on the based on the context. Okay, so this will basically be my thing. And one very important thing is that I can also specify based on the based on the question. Okay. Now, let me go ahead and define my context. So here, I will basically define my context. And this will, this will be ending over here. So let me go ahead and write it. So this will basically be my context. And here, I can give my context like this. Now, the next thing that I usually give is nothing but my question, which will be my input parameter over here, right. So this will basically be my input. Okay, so this becomes my chat prompt template. This is very good. Now, as you know, what are the next step, we have to probably use document chain. So I will go ahead and use document chain over here, then I will specifically use all the vectors in the form of as retrieval, right? So that this is just like an interface to probably read all the data context that I specifically get from this particular vectors. Now, since we are using all these things, right, like we are using this chat prompt template, we are using some context based information, then document chain retriever and create retrieval chain will also come into existence. Now create retrieval chain already I've explained in my previous video here, I'm going to combine my document chain and the retriever. So both of them will be specifically combined. And now I will go ahead and probably write my prompt prompt will be st dot text underscore input. And here I'm going to define my input, your prompt here. So guys, now next step, what I can actually do is that LLM is ready, my prompt is ready, my retrieval chain is ready, everything is ready. Now, all I have to do is that I will write if prompt, okay, if the prompt is there, I will go ahead and write my response, okay. And with respect to response, I will be using this retrieval chain. So let me go ahead and write retrieval chain dot invoke, okay. And whatever input I'm specifically giving, right, with respect to this particular input, and that should be in the form of dictionaries. Okay, and here, I'm going to write input, colon, prompt, right. So that so this is also done perfect, I'll be able to get the response over here. Now, it's always the best way that if I really want to get the response, right, I go ahead and probably use some timer out there, right, so that you will be able to see it, like how much time it is probably taking. So import time after imported time over here. Now, inside as soon as probably I go ahead and hit the prompt itself, right away, as soon as I press enter, you know, so timer should probably start over here. So I'll write start time dot process time, okay. So this will basically be my process time over here. And then I will go ahead and write print, okay, or I don't have to print also here, nothing, okay, what time it will start and all we can probably do it. But again, I can go ahead and write print response time colon, okay. And whatever is the response time, I can specifically do the subtraction over here itself, right. And here, I can just write something like this, the current time that is time dot process time, okay, process time, minus start, or whatever is the start time, right. So I can specifically get this and I'll be getting the response itself. And once I get the response, I will go ahead and write st dot right. And this will basically be my response of answer. Okay. Whatever answer I'm specifically getting. Okay. So yes, almost done each and everything over here. And I think we will be able to get all the things with respect to this. But again, it is a good idea that, you know, you also try to create an expander where you probably get all the context also what it is basically getting a trip. So a small four to five lines of code that I will be adding it over here, right with a string made expander, you can see, I'm taking up all the context from that context, I'm taking the page content and displaying it over here. Right now, let's go ahead and run this, I think it should be working absolutely fine. Amazing end to end project where we definitely learn a lot of thing. And here, finally, you'll be able to see stream let run agents. Sorry, app dot p y, right. But I think it should be running. And we'll see the speed because the speed will be the most important thing. So let me go ahead and cd, cd grok. And now same thing, I will go ahead and write. Now I feel the main the most excited thing that I will probably be excited about this application is how quickly I'm able to get it. So textplater dot split documents, take two position number three were given. Let's see. This kind of errors usually comes, but it's okay. Recursive cat splitter. Is it regarding recursive text splitter, splitter documents? Okay, where did I give three splitter documents? Let's me see in the documentation. But again, this kind of errors will definitely be coming up. Okay, here, I don't require embeddings. Okay, so one is only required. I just saw that document. So that's it. Okay, sometime it happens, but it's okay. Now, probably whatever question I asked from here, okay, let's see. I will go ahead and ask this. Now I think it should be running. Let's see. Let's see. Let's see. Let's see. It's stopping with a second. But there are a lot of content from here. So that is the reason I think it is taking so much of time. I think for the first time for the indexing purpose, it is probably taking a lot of time. Let me delete this once again. Terminal, let me do one thing. Okay, let me to just improve the performance, right? I will just not take all the documents over here. I will just take some documents. Okay. Let's probably take the initial 50 documents. That is how I'm going to probably do it. Okay. Initial 50 documents, if we are able to do that much, I think it will work out. I don't want the maximum number of files that will probably create me a problem over here. Okay. Command Prompt, let's see open CD grok. Okay. streamlet. I think it will take some time because I'm saying that as soon as the page is getting loaded, this needs to be embedded or indexed. Okay. So streamlet run app.py. Let's see if this works faster. I will not show you the other one then.
probably it will take some time, you know, because the entire website is basically getting indexed. And let's see how much time that vectors will probably get created. That is how things are basically happening over here. Because we are taking the first 50 documents, but I hardly see that 100 to 200 documents because it's a big website that this document dot smith dot lang chain, right. So you can see this is a big, big page. And there's so many things that is already available over here. Okay, let's see how things goes ahead. And okay, st dot session has no attribute vector, did you forget to initialize it? Okay, one more error. One more error, error, error, error, error. Okay, let me see. I'll store the vectors, vector vectors. Okay, vectors is used over here. Let me see. Okay, vectors. I understood that. It's okay, we can understand that. Okay. Okay, now I've again reloaded it. And it's going to take some time. Hardly, I think hardly 10 to 15 seconds, because it is going to index this entire page. And for the first time indexing it is it is going to take that much time. So guys, whenever I'm probably searching things, right, and it is giving me some of the documents in Larry search, but with respect to the summarization from the language, it is not coming up. I find out that one of the reason is that gamma seven B model, you know, it has a very less context length, you know, so if I probably consider over here, and probably seen the API access, right? There are multiple models that we have specifically used, right? One is Mr. Light into seven B gamma seven, only 8k context, 32k context length. Let's do one thing. Let's try some other model over here. Okay, the mistral model and see whether it will be improving the performance or not or whether it will be improving the accuracy or not. So you should be knowing all these things, I'm not going to edit any part of the video over here so that you know, you get an idea like how things works. And here now, let me go ahead and run this and reload it completely. And now we will try to see it, okay, how the accuracy is, but again, we are going to focus over here and see that how things are with respect to this, right. But at the end of the day, it is being able to provide this context, you know, whenever I ask any specific question based on that context. So I can just ask how to install a langsmith, how to create an API key, say how to set up an environment, something like that. Okay. So here, how to set up an environment. So let's go ahead and work it out. I think it should be there. Okay, it should be able to probably give me some kind of response, at least now. And again, let's see this part also. Okay, it's working. But I think the document similarity search and everything we will be able to see it. Okay. And along with that, you'll also be seeing many more things, right? So many things that you can probably search out of it. So let's see. I think for the indexing purpose only, it is probably taking time. So the response time is less see to set up an environment, this, this, this, all the information are specifically over here. So this looks absolutely amazing. Absolutely good. You know, and that is how you can actually do it. And just you can imagine like, what happens if you probably take an open source LLM models with a high context length, right? So definitely try it out from your end. Again, you can ask any questions as such. Let's ask one more question over here. How to create an API key, okay, how to create an API key, create an API key. And if I execute it, so here, you can actually see the responses there. And to create an API in langsmith, you can follow the following steps, all the content that is present in the website is shown here, you can probably see the entire document similarity search. So guys, I hope you are loving the entire langchain series, we have almost completed almost each and every module specifically to langchain that is actually required to create end to end projects. And we have also developed multiple projects over here. We have used both paid and open source LLM models. Now one of the most requested video was that Chris, how can we work considering langchain along with hugging face libraries, and making sure that you use only open source LLM models and try to create some amazing end to end projects. So in this video, I'm going to use langchain having face and open source LLM models like mistral that is specifically hosted in hugging face. And then we'll try to develop a q&a rag complete rag app. And I hope you will be liking this particular video, please make sure that you watch this video till the end, because there will be a lot of learnings that you will be able to find from this particular video too. So let me quickly go ahead and share my screen. So here you can actually see I've created a folder called as hugging face. And there are some US census data, which I probably downloaded from the internet in the form of PDF and I have uploaded in this specific folder, right. So we will go ahead and probably read this again, we are going to use completely open source LLM models like mistral using hugging face itself. And then we are going to create the entire q&a application. So let's quickly go ahead and do this. And initially, as you know that initially, we need to load all the data ingestion step that is loading all the libraries, all all the PDFs itself. So for that, I will be importing from Lang chain underscore community, okay, dot document loaders, import by PDF loader, okay. So this is one of the things that I will be requiring then from Lang chain, underscore community, dot document loaders, I'm also going to make sure that I use a PDF directory also for that. So here, I'm going to import by PDF directory loader, okay. And then from since I, after reading it, I need to do the character splittings, divide that documents into chunks. So for that, I'm going to use recursive character splitter. Along with this, again, as we know that we will also use some kind of vector store over here. So I'm going to use the vector store called as files. And as you all know, we also going to use hugging face. So for hugging face, as I said, all the embedding techniques also I'm going to use from hugging face itself. So for that, in Lang chain, also, you have this embedded in techniques where you have hugging face, BG embedding, which is completely an open source, and which is good enough, right. So we are also going to use it. Along with this, I'm also going to import NumPy as an NP, if I specifically require NumPy, I don't think so I will be requiring but I'm going to just use this. Okay. Two more amazing libraries that I'm going to use from Lang chain, since I will also have to make sure that I create a template. So from Lang chain dot prompts, I'm going to import prompt template. Okay, so this is also what is specifically required. And one more right one more change I will specifically require again from Lang chain dot change, import retrieval QA, right. So retrieval QA chain we will specifically be requiring. So all these libraries we have basically imported, and we are going to use this for this particular purpose. Now, let's go read the PDFs on the folder. Okay. Now, here you have loader is equal to pi PDF directory loader. And here I'm going to give my census folder, right. So here I will go ahead and write dot start us underscore census. Okay, so this census folder, I'm going to specifically read, and then I'm going to use this loader dot load. And I'm going to I'm going to basically get my documents. Okay. So once I probably get the documents, then what I will do, I will create my text splitter. And for this, I'm going to use my recursive character text splitter. And for this, the chunk size chunk underscore size, I will be taking 1000. Okay, and chunk, chunk underscore overlap will be somewhere around 200. Okay, so all these things, as you all know, I've written this kind of code a lot. And then finally, my final documents okay, will be nothing but I will be using this text splitter. And then I will split all the documents. So I will write split all the documents. With considering all my documents over here. And let's go ahead and see my final documents. First, final document, I'll just try to see I don't have to display the entire thing. So this will probably read all the PDF file inside that particular US census folder, which is present over here, all the four PDF files, and then it will probably give me a thing. So here you can see one error is there. So here, let me go ahead and write a chunk size. And I think it should be working fine. And all the PDF, all the materials will be available in the description of this particular video guys for you all of you, right. So here you can see and the total number of length of final documents you can probably see is somewhere on 316. So here you can see page content, hence insurance cover status, everything is available over here. And this is the total number of documents that I'm actually able to get it. Okay. Now, the next thing that I will be specifically requiring is the embedding technique, what embedding technique I'm going to use. As I said, I'll be creating hugging or let me just go ahead and write. See, you can use all my embedding anything, but since I am planning to do it with embedding using hugging face, so I will be using all the hugging face components. And these all are also very good, completely open source and available for you from hugging face. So here, I'm going to basically write hugging face embeddings. And the next thing that I will specifically require is my model underscore name, this is the first parameter that I will be giving. And then model number underscore name will be nothing but I have explored some of the model over here.
is this one, right by this one is also there. Okay, so one emitting technique is this one, I can also use sentence transformers. Okay, so let me just show you instead of using this, I can also use this. So I'll write down the comment over here. Oops, just a second. I'll just write down the comment over here. Okay, so this also you can specifically use, let's go ahead and use sentence transformers if you want. Okay. And if you don't want, you can also use this, no worries, it is up to you. Okay. Both or any of them can be specifically used, it is up to you. Okay. And these are some of the free available embedding techniques that is basically available. Okay. So now I have actually created this, I will go ahead and write my next one. So before that, let me do one thing. Okay, and okay, so the model name is driven, then I will be using models kW ARGS. And I know I don't have to even use GPU, right? Because with the help of GP with the help of CPU, this is possible because I have very less documents. So I will be using device, device colon CPU. Okay. So this will basically be my device with respect to this model arguments color. Okay, the next parameter that I'm going to specifically use encode underscore kW ARGS. And encode underscore kW RGS will be here, I'm going to use a parameter which is called as normalized. So you may be thinking, Chris, from where are you probably consider this, I'm just checking the documentation on the hugging phase itself, right. So if you go and search for this, right, and you will be also able to see some of the examples, right. So here you can see this, okay. So here, you just click on this. And here, you will be able to get all the information, there will be some code files also, which you will be able to see sentence transformer is also there, see, everything is over here, right, all mini, this one only I told, right. So similarly, you can probably go and check for many of if you want, okay. Now, the next thing that you really need to do is that you have to set your access token of hugging face unless or until in order to access all these models, hugging fish token needs to be there. So I will go and click on settings in my hugging face account, there'll be something called an access token, okay. And just make sure that you create this particular new token. So once you probably create it, you will be able to see it, okay. And then you can copy it and you can specifically use it. But anyhow, I will show you where we will be specifically using now if I go ahead and execute it. So here, could not import standard transformer pip installed sentence transformers, okay, I need to install sentence transformers. So let me quickly go ahead and update my requirement or txt file. See, whenever error comes, some type of things is always going to happen, but it's okay, we should not be afraid of errors, you know, errors will probably help us to do anything that we want. Okay, pip install minus our requirement dot txt. So here you can see now the sentence transformers will also get installed. And finally, you'll be able to see that everything is getting installed itself. Okay, perfect, perfect, perfect, perfect sentence transformers is coming over here. And now it will be able to you'll be able to see that okay, fun. So once it is installed, then I think then you'll be able to execute this. Okay. So we'll wait till this installation basically happens. So if you have not subscribed, and please make sure that you write something in the comment section. How are all these videos for you all? Are you able to understand I hope many, many people are able to see whatever feedback that I'm getting from people with respect to the transition, I feel that yes, the videos are doing something in your life, at least some amazing things in your life. And I hope to do as long as I can. Okay. But again, my main aim is to teach you in such a way is that you learn, learn, learn, learn things in a way that always matters in your life, right. And that is what I'm actually looking at. Okay. So let's wait. Sometime, sometime, sometime, some amount of time usually takes for this particular installation. But I think once this installation takes place, we are good to go. Okay. And I'll also show you how we can actually set up the hugging face, you have actually created a human face tokens, right. But how to set it up as an environment variable. Still, it is saying that it is not installed. Yes, installation is taking some amount of time. So let me pause the video till then I think it is going to take some time and then we will continue. So guys, the installation has taken place. Now you can go ahead and execute it. And I think it should not give me an error now and your embedding hugging face should be imported over here. Okay, so perfect. Let's see how it is going to take some amount of time again, it's going to take somewhere around at least one minute because this particular embedding is quite big. Oh, 13.1 seconds. This is absolutely amazing. And this is really good. Now after you have this hugging face embeddings, we will try to use this hugging face embeddings and try to just check on one of the file. Okay, so I'll just go ahead and write embed underscore just to check whether it is working fine or not. Embed query. And here I'm going to basically take my final underscore documents of zero. Okay, so I'll just take the first one, okay, first one. And probably I will just go ahead and use the page underscore content, because it will be available over here. Now what I'm actually going to do if I'm specifically using this hugging face embeddings, okay, I will also go ahead and import NumPy. Let's see import NumPy as np. And let me convert this into an array np dot array. Okay. I think so it should not be giving you an error. So we are just trying to see okay, perfect. So whatever was available in the first document of page content, this is how the embedding is basically shown for the entire words, okay, whatever the words are specifically present in that particular embedding. And if you probably go ahead and see this, what is the shape of this particular array? Okay, I will also print that specific shape. Okay, let's go ahead and print this first. Print this first, okay. And then we will print it. I will paste this dot shape. Okay. I think you will be able to see the shape 384. Okay. And it is also getting printed. So perfect. Now what we are going to do, if this embedding technique is working fine, then what we'll do we'll convert for the words into vectors by using this embedding technique and store it in a vector store. So for that, I already have imported fires. So I'm going to probably create my vector store. And I'm going to write my fires dot. And then I'm going to say from documents, did I import fires? Yeah, it is important from documents. And from the documents, what I'm actually going to do is that I'm going to give my final documents and final documents till let's say I give 120 records, okay, there are somewhere on 310 records. I'm not putting all because it is going to take time, okay, just to show it to you. But later on, once once you have time, you can go ahead and increase this number. And here, I'm going to basically use hugging face embeddings. Okay. So vector store will basically get created. So if I go ahead and execute this, again, it is going to take some amount of time, at least 30 to 40 seconds, because there's so many documents over here. And all this will be embedding, right, it'll get embedded into words. And as you can understand, every document will be getting converted into 384 based on the chunks that we have specifically traded for vectors, okay. So this will basically be my this will be my vector store creation. Okay. And this is what is the step that we have specifically done in every step, right? vector store creation. Perfect. So let's wait for some time. And I think it is 30 seconds it has crossed since 120. Before when I tried to do it, it took me a lot around 30 seconds because there were only 100 documents. Okay, so 43.4 seconds. Perfect. Now, we will try to use this vector store and perform query using similarity search. Okay, so similarity search will be basically done over here. similarity search will be done. Okay. And once we specifically do the similarity search, we will be able to see or we will be able to get some kind of response over here. Okay, so how to do the similarity search, first of all, I will go ahead and write my query. With respect to the query, you will be able to see, I will just write a question, what is the health insurance? Okay, now see this question that you will be saying I did not just write it by purposely or somewhere. Here, what I will be doing with what is the health insurance, insurance coverage? Okay, I what I did is that I just went to this folder, okay. And when I probably go ahead and see this folder, I will just open my first file. Okay. And this is where I saw this question, what is health insurance coverage. So I will take up this question itself. Okay, and I will paste it over here. Okay. Now, once I paste it, so this is the query that I'm actually searching, I will go ahead and write relevant underscore documents, since I need to probably retrieve that result that is present over there. So what I will do, I will use vector store vector store dot similarity, similarity, underscore search, okay, or whatever query I have specifically written. Okay, whatever query I have written. And let's go ahead and print all the relevant documents. Let's go ahead and print all the relevant documents. So once I print it, I think I should be able to
relevant relevant relevant relevant is this not the same thing okay so here you can see page 2 what is health insurance and here you will be able to see the answer these brief prevents stage level everything you are able to see this right completely right and in order to get this what I will do to exactly get the page content so I will write underscore page content so whatever question I am specifically asking you will be able to see this is the brief present state level estimates of health insurance cover is it same or not so this brief present state level estimates of health insurance coverage using data from American community source server survey I mean American community survey so here you can see the US censors and all everything is working absolutely fine okay sometime you also may be requiring you know more results right more let's say right now I'm just getting one reason let's say I want two to three results now or two to three similar kind of results now what I can actually do is that or according to that particular text of query that I've asked the most relevant document I specifically want so what I can do I can basically write this vector store okay vector store and I will specifically call as a retriever function I hope everybody knows what is as a retriever because we have discussed more different thing about it completely as retriever is just a kind of interface that gets connected to a vector store so that you will be able to get more results more amazing results based on the similarity search and here I will say search type similarity but my search underscore arguments okay will be nothing but my search underscore arguments will be nothing but k is equal to 3 k colon 3 okay so this is what I will be probably getting the vector store okay and you can probably consider this as my retriever itself right so I will save this as a retriever okay and then let's print retriever okay so here you can see fires hugging face embeddings all the things and the number of search arguments like most relevant documents I require is three okay and now I can also use this particular retriever to invoke anything that I specifically want but before I invoke understand I also require an LLM model now for those LLM model as I said I'm going to use hugging face LLM model now that is the reason the access token that are actually created right I am going to set up that access token with my hugging face so I will write OS dot environment okay and here first let me go ahead and import OS import OS OS dot environment my hugging face hub API token so I'm going to set this up as my API token because from here hugging face hub only will be downloading the model will be using the model and here we are going to specifically use my token key right I'm hard-coding the token over here because I have copied it from there itself but don't use this because I'm going to regenerate that particular token okay it'll be of no use okay now let's go ahead and if you don't know about hugging face hub guys what exactly is a hugging face and all I'll be writing a some description over here for you right so hugging face is a platform with over 350 K models 75 K datasets and 150 K demo apps all in spaces all open source and publicly available in an online platform where people can easily collaborate and build ML together so I'm also going to use this same hugging face hub and then I'm going to write from Lang chain dot community Lang chain underscore community dot LLMs import hugging hugging face hub right so I'm going to specifically use hugging face hub I will create HF is nothing but hugging face hub and now I will show you how do you can call your models right the main parameter over here is nothing but you have to give you a repo underscore ID okay now here one repo ID since we are going to use the open source model is nothing but Mistral right so this is basically a skeleton for you you can go ahead and replace the model whatever you want right so we are going to use Mistral 7b okay again it is up to you whatever model you specifically want to use only you have to give your repo ID along with this one more parameter that you really want to give is model arguments okay so in the model arguments here we will specifically give temperature parameter okay so temperature will be nothing but it will be 0.1 and the next parameter that you are specifically going to give is nothing but max underscore length okay and this max underscore length will be nothing but 500 okay so these are the two parameters that you really need to create so that you will be able to use this particular open source model understand one thing this will be directly using from the hugging face hub itself okay you don't need to download it locally for downloading if you want to download it locally and probably want to check it out so I will also give you the code for that you can which you can try it out okay now let me go ahead and write the same query so I will say query is equal to what is the health same query insurance coverage okay so this is the query that I have now in order to invoke it I will use the same h hf dot invoke okay and then I'm going to basically give my query so here you can see now I will be able to get my response and this is the most generic response right what is the health insurance cover I have just asked this is the most generic response which Mistral knows but my main aim is to probably create a Q&A app right Q&A app specifically for rag application so still it is not communicating with my data that is available in my PDF okay so right now this is just given me a general general output okay now general output what is available with respect to the knowledge of the internet now the next step what we are specifically going to do see understand if you want to make sure that we interact with our PDFs this hugging face LLM models we have to probably create our own prompt template one more thing that I really wanted to show is that if you want to download locally right hugging face models can be run locally through hugging face pipeline so here is the new code before we used to use hugging face hub here you have hugging face pipeline same code for task generation everything you can probably execute this and this will basically be able to generate it okay but I'm not going to use this since I have already time to use from this right so it is going to take some amount of time with respect to execution perfect now this is done now as I know I need to use this LLM model to interact with my PDF so for that I will try to create my prompt template okay now the prompt template will be nothing but I will I'll be use the following piece of context okay to answer the question the question asked okay so here is the simple prompt that I have used use the following piece of context piece of context to answer the use the following piece of order to answer the question asked please try to provide the answers only based on the context so I'm just giving a simple prompt okay pretty simple prompt okay now my context will be over here co n txt okay and then my question will be nothing but it will be over here and I think we have done this many number of times in my previous video okay and if I want one more thing I can probably give I will just write helpful answers okay and I'll keep it blank okay so this is my prompt template that I'm going to use and then in order to make it as a this I'm going to use my prompt template prompt template my template will be assigned to this same prompt same same prompt I think this is assigned to this prompt okay so this will basically be my prompt okay and then my input variables will be having two things one is context okay context and the other one is question oops same question just a second question okay so context and question we are setting it as an input variable and this basically becomes my prompt itself okay so I'm going to execute this okay now understand if I want to combine okay I go if I want to combine my Lang chain right my sorry not Lang chain my my template with the LLM models obviously what you really need to do so that we also get the context and the question it's create retrieval QA right so create retrieval QA chain right so if you remember that in my previous video one more way that I can specifically do is by using this retrieval QA right and in order to use this retrieval QA here what I will do I will just write from chain type okay from chain type and here I will start giving my parameters that I require first parameter is nothing
by llm. llm will be nothing but hf whatever a hugging face library that we have created. I will create my chain type as stuff. Okay, so chain type will be nothing but stuff. The next parameter that we have specifically going to do is my retriever, which will be assigned to my retriever itself, which is basically defined the return source document, you can keep it as true or false. So that we'll be able to understand from where it is basically coming up. So return source documents, which will be assigned to true. Okay. And the next parameter is nothing but it is my chain type, chain type, k, w, a, r, g, s, okay. And you can also check out this entire documentation from the from the lang chain documentation. So prompt will be nothing but it will be assigned to this particular prompt. Okay, sorry, this prompt. So this is done like most of the things have been completed over here. But understand one thing in retrieval qA, you have to give your hugging face, your chain type stuff, your retriever, return source document so that you need to understand from where it is coming. And chain type arguments is nothing but with respect to the prompt. So this prompt will return some of the documents, it will be setting up in the context, then the question will go over there, and entire this retrieval qA chain will work. Okay. So here, I'm going to write it as retrieval qA, right. So this will basically be there. Let me quickly go ahead and execute it. Okay. Now, it's final that you just need to run it in order to run it here is the question, right. So now finally, we can use this retrieval qA that we have actually created and we are invoking with respect to the query that we have already written what is the health coverage. And finally, let's go ahead and check our results. So here you can see what is health insurance covers the brief presents the state level estimates of this American Community Survey, the US Consulate, US Census Bureau conducts ACS throughout the year, the service asks response to report the coverage at a time. So every information is specifically given. Now you can go ahead and ask some other questions also, if you want. Let me go ahead and ask this differences in the uninsured rate by state. Okay, so I'll go ahead and ask this, let me go ahead and write my new query. So query is equal to let me just go ahead and close this in. Okay, so here is my query. So let me go ahead and execute it. Now if I go ahead and see this, it should be able to give me the response. So here is a comparison of ACM measure of health insurance. So everything is probably matching over here. So you're unsure the rates of this, this, this is coming up. Okay, so all the information are specifically coming up from here. This is perfect. Now we have, I've also shown you how you can probably work with Lang chain and hugging face. So let me go ahead and update this also if you want, okay. Insert cell chain sale to insert. Okay, so perfect. This is it. I hope you like this particular video. At the end of the day. The one thing that I really want to say is that more you practice things more you see things definitely will be able to achieve multiple things over here. So yes, I hope you like this particular video. This was it from my side. I'll see you in the next video. Have a great day. Thank you and all take care. Bye bye.
So this will be my next. See, if I'm giving a system prompt, I also have to give a user prompt right user prompt will be whatever question I ask. So this will be user. And here I will define something like question, colon question. I can also give context if I want. But right now I'll just give it as a question, a simple chatbot application, so that you will be able to start your practice of creating all the chatbots. So now I will go ahead and define my streamlit framework. Okay, see, the learning process will be in such a way that I will try to create more projects and use functionalities that are there, right. And then this way, you'll be able to work in an amazing way. Okay, so here I'm going to basically write st dot title, land chain demo with the opening API, st dot text and was going to put search the text topic you want. Okay, now, let us go ahead and call my open AI LLMs. Okay, open AI LLM. So here, I'm going to basically write LLM. And whenever we use open AI API, so it will be nothing but chat open AI. And here, I'm going to give my model name, the model name will be nothing but GPT, GPT 3.5 turbo. So I'm going to use turbo because the cost is less for this I've put $5 in my opening account, okay, just to teach you. So please make sure you support so that I will be able to explore all these tools and create videos for all of you. Okay. And finally, my output parser, see, always remember, land chain provides you features that you can attach in the form of chain, right. So here are three main things we have created. One is the chat prompt template. Next one is the LLM. And next one is the output parser. Obviously, this is the first thing that we require. After this, we integrate with our LLM. And then finally, we get our output. So string output parser is responsible in getting the output itself. Finally, chain is equal to we will just combine all these things. So here, I'm going to write prompt LLM. And then finally, my output parser, right, I will show you going forward how we can customize this entire output parser and all. And finally, if I write if input text, if input underscore text colon. Now, whenever I write any input and probably press enter, then I should be able to get this output. So st dot right and here I'm going to just write chain dot invoke. And finally, I get I give my input as question and that input is assigned to my input text, input text, right. So this is what we are going to basically do, right st dot right. Now, this is what we are doing a simple chatbot application. But along with this, we have implemented this, this, this feature is specifically for Langsmith, Langsmith, Lang, Smith, tracking, okay, this will be amazing for to use. Okay, and this is the recent updates that are there. So whatever code I'm writing will be applicable going forward in various things that are probably going to come up. Okay, now let's go ahead and run this. So in order to run it, you will just need to write nothing but streamlit run app dot p y. Okay. Oops, there is an error app dot p y. And here I'll do allow access. Okay. So right now, you will be able to see over here Lang chain series test LLM. But my my, my project name was project one. Okay, so now if I go ahead and hit, hey, hi, okay, and just press enter, you will be able to see that we will be getting this information over here. And here you can see my project something let me reload it. Tutorial one, right. So this is the first request that is already been hit. And here you will be able to see your enable sequence chat prompt template, right all the chat prompt template output message you are a helpful assistance plays a response to the user queries, right? The along with this, you will be seeing chat open AI API. And with respect to this, what was the cost everything you are able to track so point 00027 dollars is the cost that actually took with respect to this. And finally, my string output parser, how can you assist today with respect to this output parser, it is just going to give me the response clearly. Now when I develop my own custom output parser, I will be able to track everything. So here what you are able to do, you are able to monitor each and everything that is there, right? All the requests that is probably coming up. Okay, so provide me a Python code, Python code to swap two numbers. Okay, so once I execute this, and here you will be able to see that I'm able to get the output and answer everything is over here. And for this, you will be able to see the cost will be a little bit high. Okay, if you don't agree with me, or let's see with respect to tutorial one, the second request that I've actually got 4.80 seconds, yes, it took a little bit more time. And here the cost was point 000211. So it is based on the token size, right? For every token, it is bearing some kind of cost. Perfect. This was the first part of this particular tutorial. Now let's go to the second part. The second part is more about making you understand that how you can call open source LLMs in your local itself and how you can actually use it. So for this, first of all, I will go ahead and download olama. Okay. olama is an amazing thing, because you will be able to run all the large language models locally. The best thing about olama is that it automatically does the compression and probably in your local, you will be able to run it. Let's say if you have 16 GB RAM, you will just have to wait for some amount of time to get the response. But llama to encode llama, you can specifically use it over here, all the open source LLM model, it supports a lot of open source LLM models. And yes, in lang chain ecosystem, the integration has also been provided over here. So what I'm actually going to do over here is that I'll show you first of all, just go ahead and download it. This is available both in Mac, Mac, Linux, and Windows wherever you want. Just download it after you downloaded it, what you really need to do is just go ahead and install it. It is a simple exe file for Windows MSI file for Mac OS and Linux is a different version. So you just need to double click it and start installing it. Once you install it here, somewhere in the bottom, this olama will be start running. Okay. Now once olama installation is done, now what I will do over here, I will create another file inside my chatbot. Okay, and create another file, local llama. Okay, local llama dot p y. Now local llama dot p y, what we are going to basically do over here is that with respect to the local llama, I will first of all go ahead and import some of the library C code will be almost same right there also I'll be using chat open API chat from template string output parser. So I'll copy the same thing over here, I'll paste it over here. Now along with this, what I'm going to do, I have to import olama, right? Because that is the reason why we will be able to download all the specific models. Okay, so Lang chain community dot llm see over here, whenever we need to do the third party integration, so that will be available inside Lang chain community. Okay, so olama is third party control configurations. Let's say you're using some vector embeddings that is also third party. So everything will be available over here. Okay. Now this is done to Lang chain underscore community dot lm import olama and then we have this output parser string output parser core dot prompts that is nothing but chat from template and everything is there. Okay, now let's go ahead and write import stream late as st. So I'm going to going to use the stream late over here, along with this import OS. And not only that, we will also go ahead and import from dot ENV. Import load underscore dot load underscore dot ENV. Okay. Now we'll initialize it load underscore dot ENV. Okay, once we initialize all this random way, all this environment variables, as usual, I will be importing these three things. Now see, in my previous code, when I was using open API, prompt template, we have written it over here, right? Same prompt template, we'll also write it over here. Because we just need to repeat it. Because the main thing is that you really need to understand how with the help of olama I can call any open source models. Okay, so here it is. And then finally, you will be able to see where is my code to call my open AI LLMs that we are going to see over here. So this is done stream late framework also, I will try to call it over here. Okay, it's more about copy paste, the same thing that we have actually implemented. And then you will also be seeing this is the code that we are going to implement it. Okay. But here we are calling chat open AI. Okay, I specifically don't want chat open AI. Instead, I will be calling olama. Okay, so olama, whatever library we have imported, so olama, okay. And then here, we are specifically going to call a llama. Okay. Now, before calling any models, now, which all model are specifically supported, if you go ahead and see in the GitHub, right of olama, you'll be seeing the list of everything, every, every, every libraries that it supports, like llama to mistral, dolphin, five, five to neural chat, code llama, all are mostly open source, gamma gamma is also there. But before calling this, what you really need to do is that just go to a command prompt, let's say that I want to use gamma gamma model, okay. So what I have to do, or I have to use llama model, right. So in order to do this, I have to just write olama run whatever model name
because initially it needs to download it, right? This will get downloaded from some open source, some GitHub, it can be GitHub, it can be HuggingFace, somewhere, right? Some location there will be there, we have to download that entire model. So let's say that I want to go ahead and write olama run gamma. So this, what will happen? It will pull the entire gamma model, right, wherever it is. So here you can see pulling will basically happen. Now, this is right now 5.2 GB, right? For the first instance, you really need to do it. Now, since I am writing the code with respect to llama2, I've already downloaded that model. So that is the reason I'm showing you another example over here, olama run gamma. Now, once this entire downloading happens, then only I'll be able to use the gamma model in my local with the help of olama. So I hope you have got an idea about it. Now, what I'm actually going to do, so here I've called olama model llama2, okay? Then again, output parser is this, and I'm combining prompt llm on output parser. And everything will be almost same, and that is the most amazing thing about lang chain. The code will be only generic. Now, only you need to replace open AI or paid or open source, it is up to you. Again, I'm saying you guys, the system that I'm currently working in has a 64 GB RAM. It has NVIDIA Titan RTX, which was gifted by NVIDIA itself. So with respect to this amazing system, I will be able to run very, very much quickly. That is what I feel. So let's go ahead and run it. So here, what I'm actually going to do, I'm going to write Python. So it is Streamlet. So Streamlet run local llama.py. So once I execute it, here you'll be able to see now. Now, instead of open AI API, I should add, okay, no module name, lang chain community. Let's see where is lang chain community, okay. I have to also make sure that in my requirement.txt, I go ahead and use this lang chain community, and I need to import this library since I need to do that. And that is the reason I'm getting an error. So if I go ahead and write pip install minus our requirement.txt. Oops, cd dot dot. Okay, now if I go ahead and write pip install minus our requirement.txt. So here you'll be able to see my requirement.txt will get installed. This lang chain community will get installed. Once I'm done with this, then I can probably go ahead and run my code, okay. So this will take some amount of time. So if you are liking this video, please make sure that you hit like. There are many things that are probably going to come up and it will be quite amazing when you learn all these things, okay. So once this is done, then what will happen is that we can, and you can use any model, up to you, okay. And I don't want this open AI key also, only this two information I specifically want. I'll be able to track all these things, okay. And later on, I'll also show you how you can create this in the form of APIs. Again, it'll sometime, it'll take this. But let me know, how do you think all these tutorials are? Lang chain, I see a lot of purpose for this particular library. It is quite amazing that people are doing, the company is doing amazingly well in this open source world. And it is developing multiple things over there. So now I will go ahead and write cd chatbot. I will go inside my chatbot and then I will run this python local llama.py. Once I execute this, now I don't think it should be an error. Okay, it should be streamlet, come on. Streamlet run local llama, oops, local llama.py. Not python run, streamlet run. Now here you have, again, I'll be getting open AI text over here. Let me change this also so that I can make it perfect. With llama2, okay. So I've executed it, saved it, I will rerun it. I'll say, hey, hi. So once I execute it, you'll be seeing that it'll take some amount of time in my system even though I have a 64 GB RAM, but I'll get the output over here. So assistant says, hello, how can I help you today? Now if I probably go ahead with respect to this dashboard, let's see where it is. So now tutorial one, you'll be able to see that this will increase, okay? There will be one more over here, right? I've reloaded this page, okay? And you'll be able to see it, okay? You'll be able to see the new oh llama request. See, hey, hi, 4.89 second, token 39, but there is no charges because it is an open source model, right? So here you'll be able to see if I extend this, there you'll be able to see chat prompt template, oh llama, oh llama is over here. Now this oh llama is specifically calling llama2 over there and whatever open source libraries that you specifically want. Just to call this, it is very much simple. You have to just go into GitHub and download any model first of all, just by writing oh llama run, that particular model name. And once it is downloaded, it is good that you can probably go ahead with and use it. Now I will say, provide me a Python code. Python code to swap two numbers, okay? If you want more coding well chat bot, you can directly use code llama if you want, okay? So here you can see all the examples are there and this was quite fast, right? So this is good, you know? So if you have the right kind of things, so here you can see four seconds it has probably taken. Okay, oh llama is over here. All the information is probably over here, prompt and completion and all, right? Hello guys. So we are going to continue the language series. Already in our previous video, we have already seen how to create chat bots with the help of both OpenAI, API and open source LLM models like llama2. We have also seen what is the use of olama, how you can run all these open source model locally in your system. And along with this, we also created multiple end-to-end projects using both of them. Now what we are going to do in this specific video, one more step which is very much important for our production gate deployment, that is creating APIs. You know, for all this kind of LLM models, we will be able to create APIs. And through this, you will also be able to do the deployment in a very efficient manner. Now how we are going to create this specific APIs, there is a very important component which is called as LangServ in LangChain. We're going to use that. Along with this, we are going to use FastAPI. And not only that, we'll also create a Swagger UI, which is already provided by LangChain, the LangServ library that we are specifically going to use. Now it is important guys, you know this specific step because tomorrow, if you are also developing any application, you obviously want to do the deployment for that particular application. So creating the entire API for this application will be the first task that will be required. So yes, let's continue. Let's go ahead and discuss this entire thing. First of all, how we are going to go ahead, first of all, I'm going to show you the theoretical intuition, how we are going to develop it, and then we will start the coding part. So let me quickly go ahead and share my screen. What we are actually going to do over here, over here, you have seen that I've written APIs for deployment. So if I consider this diagram, this is the most simplistic diagram that I could draw. Now let's consider, see, at the end of the day in companies, there will be different, different applications. These applications are obviously created by software engineers. It can be a mobile app, it can be a desktop app, web app, and all. Now for this particular app, if I want to integrate any foundation model or any fine-tuned foundation model, like LLMs and all, so what I really need to do is that I need to integrate this with the form of APIs. At the end of the day, see, what we are going to do over here, this side is my LLM models itself, right? It can be a fine-tuned LLM model, it can be a foundation model. I want to use those functionality along with my web app or a mobile app. So what we are doing over here is that we will create the specific APIs. Now these APIs will be having routes, okay? Routes. And these routes will be responsible whether we have to probably interact with OpenAI or whether we have to interact with other LLM model like Cloud3, Cloudy3, or Llama2 open source model. So any number of LLM models, whether it is open source or whether it is paid API models, specifically for LLM, we can definitely use it. Now this is what we are going to do in this video. We will create this separately, we will create this separately, and at the end of the day, I'll also give you an option through routes how you can integrate with multiple LLM models, right? So I hope you have got an idea with respect to this and any number of LLM models that may probably come, you can probably integrate it. At the end of the day, whichever model is suitable for you, you can use for different, different functionality. And the reason why I'm making this video, understand one thing guys, because in LLMs also you have different performance metrics. Some model is very good at some performance metrics over there, like MMLU, other metrics are definitely there. So this is an option where we can use multiple LLM models. Now what we are going to do, quickly I will go ahead and open my code document. So in my previous video, already you have seen, I had actually developed till here, right? I have basically, if you see over here, we have created this first folder that is Shadbot. And inside the Shadbot, we had created app.py, then local llama.py, right? We did this entire thing. And as I said, every tutorial, I'll keep on creating folders and developing our own project over here. Now let's go ahead and create my second folder. And this time, this will be APIs, okay? So I'll just go ahead and write something like API, okay? Now with respect to this API, as I said,
Whatever we do with this local llama or app.py, right over here, I've used OpenAI API key here I've used open source models. So we'll try to integrate both of them in the form of routes, okay, so that we'll be able to create an API. So let me quickly go ahead and write over here app.py. So one will be my app.py, which will be responsible in creating all the API's. The second one will specifically be my client.py. The client.py in this specific diagram is just imagine like this one web app or mobile app, because we are going to integrate this API's with this mobile app or web app. Okay, so quickly, let's do this. First of all, I will go ahead and start writing the code in app.py. Before I go ahead, we have to make sure that we need to update all the requirement dot txt. Almost all the libraries have installed it, but I'm going to install more three libraries. One is Langsurf, FastAPI and uvcon. Okay, since I'm going to create my entire swagger documentation of API using FastAPI, right. So all these three libraries, I'll be using it. So first of all, I will go ahead and install all these libraries. That will do once we run the code right now, let's go ahead and write my app.py code. Okay. Now, as usual, first of all, let me just open my terminal. Okay. And let me do one thing with respect to the terminal, I will go ahead and write pip install, pip install, minus our requirement dot txt. Okay, first of all, I need to just try to see the dot dot, okay, then I'll clear the screen. And then go ahead and write pip install, minus our requirement dot txt. Now here, you'll be able to see that my entire installation will start taking place. And here are the other three packages that I have actually written right Langsurf and all that will get particular installed. Okay. Now till then installation is basically going on, let me go ahead and write my code. So I will write from FastAPI import FastAPI. Okay, so this is the first first library that I'm going to input along with this, I have to also make sure that I create or I import my chat from template. So from Lang chain, since we are going to create an entire API in this, okay, dot prompts, import chat from template, right. So this is done, then from Lang chain dot chat underscore models, import chat, open AI. So this is the next one, since I need to make sure that I need to create a chat chat application. So that is the reason why I'm using this chat models. Okay. This is the next library that I'll be going ahead and importing. Along with this, I will also use Langsurf, which will be responsible in creating my entire APIs, right. So from Langsurf import add routes, okay, so through this, I will be able to add all the routes over there, right, whatever routes, suppose, one route will be that I need to interact with my opening API, one, one route will be to interact with the llama two and all. So based on that, I will go ahead and import this. Next thing is import uvcon. Okay, uvcon will be required over here. Okay, next is import OS. See, I can probably enable my GitHub copilot. And I can probably write the code, but I don't think so that will be a better way. I usually use this AI tool, which is called as blackbox, so that it will help me to write my code faster. And, you know, it also explains about the code. So I'll probably create another video about it. Okay. So how to basically use this, then one more thing that I really want to import over here is my olama. So from Langston and scope community dot l lm dot l lm import. Oh, llama. Okay. So this is done. All the libraries that is specifically required by my code, I've actually written that, okay. And these are all the things that I will require to create my open API. Now this is done. Now what I'm actually going to do over here is that I'm just going to write OS dot environment. And first of all, I will initialize my open AI API key. So I will write open AI underscore API underscore key. Okay. And this specifically, I will load it from OS dot get ENV. And then I will go ahead and write open AI underscore API underscore key. Okay. So this is the first thing that we really need to do. Before I do this, I will just go back to my app dot py. And I will just initialize this. Okay, load underscore dot ENV. Let me quickly copy this entire thing and paste it over here. And I will initialize this load underscore dot ENV. Okay. So this will actually help me to initialize all my environment variable. I've also loaded my open AI API key. Now let's start this fast API. Now in order to create the fast API, I have to create an app here I've given title Langton server version 1.0. And the third information I basically want is a description. Now after this, I can use this app and I can keep on adding my routes. Okay, so what I will do add underscore routes and this routes is basically to add all the routes over there right for so the first time when we are adding this particular route, so you have to make sure that I give all the information like whether I'm going up with my app, let's say I go with this chat open AI API, chat, open AI, open AI, okay. And the third information that I will probably give is my path. So this is my one of my one of my route, you can just consider this open AI route. And this is my model that I will specifically be using. So this is just one way how you can actually add route, okay. But let me just add some more things. Because see, at the end of the day, when we created our first application, we used to combine prompt LLM and output parser in this specific way. So what we will do over here is that when we are creating routes, we also need to make sure that we add all the routes in such a way that I also integrate my prompt template with it. Okay. So here, I'm going to say model is equal to chat, open AI, chat, open AI, and I'm going to initialize this particular model. And then let me go ahead and create my other model also see Olama, I will just use my llama too. Okay, so this model also, I need to basically create it or call it okay, because I want to use multiple models. So here, I'm going to write LLM. And here Olama and this will basically be my model is equal to llama too. Okay, so I'm going to use the llama to model. So this is my one model over here. This is my another model over here. Okay. Now let me go quickly go ahead and create my prompt one. So my prompt one will be my chat prompt template, chat prompt template, dot from underscore template. Okay, and here, I'm going to basically give one chat prompt, okay, let's say one of my interaction one, I want to use open AI API. For opening API, let's say I want to create an essay. So I'll say, write me an essay, write me an essay about a specific topic, that topic, I will be giving it okay, about some topic, okay, some topic, okay. Around with 200 words or with 100 words, okay. So, this is my first prompt template. Okay, I'm saying this will be my prompt template, write me an essay about whatever topic I give with 100 words, okay, something like this. So this, let's go ahead and write this, this is my first prompt template, then I will create my second prompt template. Okay, this is important, just just hear me out. Okay. And this prompt template will be responsible in interacting with my open source model. So let me say, write me a poem, I'll say, write me a poem about this specific topic with 100 words. So, in short, this first prompt template will be interacting with this model that is shot open API, prompt to will be interacting with LLM model, okay, whatever LLM model I've written over here, that is llama to now let me go ahead and add this route. So here I'll say add routes. Okay. And again, as usual, I will write app. Now, here you see, I will be combining prompt or model, right model will be chat open API. And you know, this prompt one is specifically for my chat open API, right? Then I will also give the route path. So my path will be something like this. Okay. And this will be my API, right? So here, say, let's say I'll write slash essay, that basically means this path is responsible in interacting with the open AI API. And this is denoted by slash essay. So that API URL that you will be getting will be ending with slash issue. Okay, the other route and I can keep on adding any number of routes as you want. So another route will be like with prompt to Okay, and here I'm going to basically use my LLM.
And let me just go ahead and write this as slash poem. Okay, slash poem. So this is my another route, this is my another route. Okay. And finally, I've created two API's in short. And now here I will write if underscore underscore name underscore underscore is double equal to underscore underscore main underscore underscore, right. Now what we are going to do over here, this is the starting of the application, I will say uv con dot run. And here I'm going to use my app, comma host will specifically be my local host. See, here I'm giving you the local host. This application, you can run it in any server that you want, right. So that is that is saying that is the reason why I'm saying this is the first step towards building that production grade application. Okay, and the port number will be 8000. Perfect. So this is done, I think it looks really, really amazing. We have written the complete code over here. And this is my entire app dot p y with all the API. So that basically means if I see the diagram over here, this API is part I've actually created routes along with prompt template I've created, and I've used two API's, one is open AI, and one is llama to two routes I've specifically used. Now my time is to basically create this web app and mobile app. Okay, and I get here, I will just try to create a simple web app. The reason I why I'm creating the simple web app, because it will be able to interact with this. Okay. Now what I will do, let's go ahead and run this and see whether everything is running fine or not. Okay. So here, what I will do, I will go to my CD, which folder this is, this is basically my API folder. Okay, so CD API. Oops, port. Let's just write it very port. Okay, done. CD. I'll save this CD API. Okay. And then here, I'm going to write Python app dot p y. Okay. Now, once I execute this, from Langton, Langton dot prompts, just let me see the documentation, whether it is correct or not, it should be prompts. Okay. So, a small issue over here, it should be prompts. Now, let me just go ahead and execute it. Now, I think it should work fine. Ah, so here you can see import error. SSE starlet. Okay, so this is one of the dependency that is probably coming up. I will quickly go ahead and update my requirement dot txt. Okay. And let me do one thing. Just let me write pip install this particular application. Okay. So I will go right pip install. Control V. Oops, pip install. SSE starlet. Okay, so once this installation will probably happen, I think now we'll not get an error. Now if I write Python app dot p y, it should run I guess. So it is now running. Okay. So let's go ahead and open this localhost. Now here you can see I'm getting details not found. But if you really want to see that entire API that is basically created, you can just write slash docs. Now here you can see your entire Langton server. See, I've created open AI input schema, everything is given essay is also there poem is also there and all right. So all the API is almost created. See here you can see what is the input, what is the output output schema, what is is basically required, everything is probably given over here. And this is basically called as a swagger UI documentation. Okay, the entire swagger UI documentation is done. Now perfect. Your API spot is basically created. Now how do we interact with this way API is that is the most important part. So for that, what we are going to basically do, I'm going to let let let let let let this terminal be hidden over here, I'm going to create my client dot p y. Okay. Now with respect to the client dot p y, what we are going to do and let me do one thing till then I will go ahead and disable this extension. Okay. Because I don't want this extension to disturb things. Okay, disable. Perfect. I will show you this extension, it is a very powerful extension. Okay. Now this client dot p y is my app, it can be my app. Okay, so first of all, what I will do, I will go ahead and import requests. Okay. After importing request, I'm going to write import stream late as stream late as st. Okay, so two libraries, I'm going to import request and streamlet. Understand I'm creating a web app, I can create a front end that will interact with the API. Okay. So here, first of all, I will write two definition, the two important function, one is one is get underscore open AI underscore response. And here I'm going to basically go ahead and write my input underscore text. Okay. Here, I will call I will probably call use this request object and create my response. Now, in order to create the response in order to call the API, how do I do it? So here, I'm going to write request dot post. And how to basically call it. So first of all, I need to give the URL. Understand we are already running this URL, right? This entire local host, if you probably see over here, everything is running over here. Right? If you if you see this, okay, if you probably see this and try to try to see this entire URL, okay, you can directly see that in order to call this API, I will be using a URL that is already provided by this particular swagger UI. That is nothing but let's say I want to call the essay URL. Okay. Now, in order to call the essay URL, which is using the open AI API, here you can see HTTP localhost 8000 SS slash invoke. So this is my entire API URL, which will be responsible in calling this. And here, we also know that we need to give one input. So what I will do, I will go ahead and write over here as JSON is equal to and in the form of JSON, I will give one of my input. So input, colon, okay. And here I will write topic, why topic because see here what we are given over here, if you see a topic is there, right? Now, this topic will be nothing but whatever input text I am giving to this particular function, that same input text will be visible over here. Okay. So this is what we have actually done over here, right? So input topic colon input underscore text. So this is what is the input I will give. And it will hit this particular URL along with this particular input post. Okay. And then finally, I get my response. Now what I will do, I will go ahead and take this response, convert into a JSON. And it has a key dictionary value pairs, which were my output content will be present inside this variable called as output. And one more thing that I require is basically content. Okay. So inside this key, my entire response will be available. The next thing that I'm actually going to do is similar with the help of Olama response. And here we are going to use llama too. So get Olama response input text again request dot post. Now instead of using this slash essay slash invoke, we are going to use slash poem slash invoke, why slash poem slash invoke, because slash poem will be responsible in interacting with llama too. Okay. And then I get my JSON again, I get my response. Here I'm writing response dot JSON of output. Okay, so these are my two functions that I've actually created to get the response. Now what let me do one thing. Now let me go ahead and create my streamlit framework. Very simple now. Now from here, everything will be simple. Okay, so st dot title, this, this is there, I've created two text box, one is input text, and one is input text one. First test box, I'm saying write an essay on whatever topic, the second text was I'm writing write a poem of any other topic. Okay. So first one will be interacting with the open AI application, the second one will be interacting with llama too. Okay. And finally, I'm going to call this particular two function that I've created. If input dot text, if I write any text in the first time, then it should basically interact with the open AI response, whatever response I'm getting, it will display it over here. If input text one, I'll just call the olama response, which is interacting with the llama to in this particular URL, okay. And then I will get the over here the output. So this is my entire application, which is interacting with the API, which is already running over here. Right. Now, with respect to this client dot p y, what I'm actually going to do, I'm going to create another command prompt and let me run this. Okay. So here, I will say, Konda, deactivate. And I will say Konda, activate VNV. Okay. And let me just go ahead and run this. Now see Python, client dot py. Okay. Okay, sorry, it should be streamlit. streamlit. Run client dot p y. Okay. So this is my client side. Okay, file does not exist. Okay. Oh, I will say CD and I will go to my folder that is API. Now let me go ahead and write streamlit run client dot p y. Now see this. This is my swagger to UI, the API is running over here. And this is my front end application, which is basically in now it will interact with this API. Okay. Now, if I write the first one, the first
the first anything in the first text box, this is going to interact with the open API. So I'll say write an essay on machine learning. Okay, so here you can see name get open AI response is not defined why this is the error. Let's see. Open AI response. Oops, just a second. This function name is does not match. And this also olama response does not match or what? Okay, now I think it should work. Let's reload it. Okay. Now you can see automatically machine learning is a powerful technology that enables computer this this this. So the first text box is interacting with the open AI API, write a poem on machine learning. Let's say if I give the same thing, if I execute Now this is going to interact with the llama to say machine learning is a field of audit that allows computers to make decision or still running. Sorry. This was the first response that I got from there. Okay, it's running, I guess let me reload it. Write a poem on machine learning. Okay. And I can also write my own custom prompt if I want everybody I was thinking that we could make a bit of interesting by adding some extra challenges. So and so so what do you say something something is given away. And if I probably see over here with respect to the prompt input text one, this is also going over here. This is my input text. Okay, perfect. So everything works fine. And here you'll be able to see write a poem about this particular topic with for a five year child. Okay, now let's see, I'll just try to change the prompt and see whether it is working fine or not. So this has got reloaded it. Let's see whether this is reloaded or not. Now if I go ahead and reload this, write a poem on unicorn. Okay, I'm just writing something. Unicorn I think it should be able to give the poem. shimmering coat in various colors. Yeah, and it was that described by unicorn. Okay, something something is information is basically coming and this is basically interacting with the entire llama to Hello, guys, we are going to continue the lang chain series. And in this specific videos, and in the series of upcoming videos, we are going to discuss about rag pipeline. Now rag, the full form is retrieval augmented generation, one of the very important use cases you may probably solving when you specifically use LLM models. And most of the use cases right now in companies are demanding this kind of skill sets, you know, where you can actually develop rag, let's say you have a set of documents and you should be able to query that specific documents and get the result quickly. If you have different kinds of files, let it be a readme file, a text file or different data source file, you should be able to read it convert that into a vectors and able to retrieve any queries specifically from those kinds of data sources. So as I said, we will be implementing this completely from scratch from basic to advanced in this specific videos will understand the entire architecture and then we will do a lot of practical intuitions. Okay. So in any rag pipeline, these are the major components that will probably exist. So the first component is specifically called as load data source. Now whenever we say load data source, initially, we may have see rag is all about querying from a different different data source altogether, right. So we may be having files that may be having PDF, it can be an MD file, readme file, it can be an Excel file, it can be a txt file, it can be a database file, it can be different different files altogether, right. And in the first first step that we see over here, it is basically called as load source data. This step is also called as data injection, okay, data ingestion. Now in lang chain, the most amazing thing is that it definitely have a lot of different kinds of data ingestion tools, which will be able to load the data in various manner. So in our current video, we are going to implement each and every of this component from data ingestion till the query vector store, okay. Then, after ingesting the data, you can probably do load, you can transform and you can embed, okay, we will discuss about this what exactly is load transform and embed. Specifically, when we do loading, that is nothing but we are reading from a specific data source, then we perform some type of feature engineering over here, if we want like in transform stage, here, the data, the complete data will be broken into smaller chunks, okay. Now, why do we divide this data into smaller chunks, it is very much important to understand, because whatever LLM models we specifically use, right, is definitely has some kind of context size, right. So based on this context, it is always a good, good way to basically convert this entire data, which can be of many number of PDFs, it can be many number of pages in the specific PDF, we will try to divide this particular thing into chunks of data. So this is also what we're going to see in the practical way. Then finally, we go ahead with embeddings. embeddings basically means how we can convert all these chunks into vectors, okay, how we can actually use vectors in order to probably convert these chunks, right. And finally, all these vectors will be further stored in some kind of vector store database. Okay, so it can be a database over here. And the main reason of this specific database is that we will be able to query this particular database in an efficient manner, right with respect to any query that we have, right. So whatever vectors are basically stored, if I have a query, if I hit the query on this particular database, I should be able to get the result based on the context of the query. So this is the entire rack pipeline that we specifically use. In the upcoming series of videos, we are going to use different different vector databases in different different clouds also. Okay, so right now, we will try to implement all these things that you can actually see in front of you. And we will follow this entire architecture. So let me quickly open my VS code. And once I probably start my VS code, I will be I will continue with respect to the same thing that I've actually used, right all the projects that I've actually created. As I said that all the things will be given in the description of this particular video with respect to the GitHub link. Okay, now I've created a folder which is called as rag inside this, I will create one IP y nb file, okay, initially, we'll go with IP y nb file, and then later on, we'll try to create an end to end project. Okay, so here I will say simple rag dot IP y nb. Since we are doing it completely from basics, it is always good that we try to do it completely from basic itself, right. So this is the thing. So first of all, I will go ahead and see whether in my terminal, whether I have that IPI kernel installed or not, okay. So I will go ahead and write pip install IPI kernel IPI kernel is specifically used to install the Jupiter notebook kernels itself, right. So once I probably install this, you will be able to see that this installation will take place. And I think it is already satisfied. So it is good enough, we can probably go ahead and start the coding over here, then I will go ahead and select my kernel. And it is nothing but 3.11 0.0, right, this is what is the environment that I have created. Now, if I go ahead and execute something, so it is giving me some kind of error. Okay, so perfect. Now what I'm actually going to do, I can see that there is some kind of message over here, I will just try to disable this, I've already uninstalled this extension, so that it will not give us any disturbance. So let me do one thing, let me just open over here for you. And here I will open this particular folder. And then I will go ahead and close this, okay, close window. Okay. And now let me quickly open the VS code again. And here is my entire project, we will just try to execute it once again. Okay, perfect. So that so that I don't have any disturbance with respect to that. Okay, now perfect. Now this is done. Now let's go ahead and start our coding. Initially, I will try to show you multiple data ingestion steps. Okay, so data ingestion basically means, let's say I want to read from a text file, I want to read from a PDF file, I want to read from a web web page, how can I actually do it? Okay, so all those things we'll start to discuss. So first of all, I will go ahead and import from line chain, underscore community dot document loaders not document loaders has all the techniques. Specifically, if you want to load, let's say a PDF, you want to load an Excel file, you want to load a txt file. So over here, all the libraries will be present. So from line chain, underscore community dot document loaders. Until then, I will just close this terminal because I will not require over here, then the first one that I'm actually going to discuss about text loaded itself. Now over here, what I'm actually going to do, I'm going to create a loader is equal to text loader. And after creating this text loader, here, I'm going to give a text file, okay, speech dot txt. So we'll just go ahead and see whether we have the speech dot txt file or not. No, I don't have it. So let me go ahead and create one speech dot txt file. Okay. And with respect to this particular file, what I am actually going to do, I'm going to put some content. So I have already have that content, let me just quickly copy and paste it. This is one of the most famous
speech that is available in some history, it has been taken over here. So I just googled it and I bought this particular speech over here and saved it in speech dot txt. Okay, then what I will do, I will go over here and read this speech dot txt. Okay. Now, just by using this text loader, what will happen is that you will be able to read the speech dot txt, then I will use loader dot load. And here, I'm going to specifically convert this entire thing into text documents. Okay, once I write loader dot load, that basically means it is going to just convert this into a text documents. And this finally, you will be able to see my test documents. And I will go ahead and execute it. So here, you will be able to see that once it is reading the specific speech, it has the entire text document. Now it is becoming so easy just to read the txt file. Okay. Now, the next thing that I'm going to specifically do is that since I'm also going to use open AI, API keys, or I'm also going to use Olama embeddings or Olama model. So what I can do is that I will just go ahead and import OS. The next thing that I will go ahead and do is load underscore dot ENV. Okay, so load underscore dot ENV. So for this, I'm also going to import from dot ENV. Import load underscore dot ENV. Okay. Now why I'm doing this so that I will be able to call all my environment variables. And in one of the environment variables, I also have my opening API key. So now I will go ahead and write OS dot environment. And here, I'm going to use my open underscore a, sorry, open AI underscore API, underscore key. And here, I'm just going to use OS dot get ENV. And here again, I'm going to use my open API key. So this is basically going to call the open API key from the environment variable. Now a very important step altogether over here, guys, now is the main term that we will be starting to see some more data ingestion techniques. So one more data ingestion technique is directly reading from the web based web based loader, you can basically say, okay, and for this, again, I will be importing understand any document loader will be available in this specific library itself. And instead of writing text loader, we will be using web based loader. Okay. Now, along with this, I will also import bs for that is beautiful soup for so for that also what I will do, I will just import over here in my requirement dot txt, bs for so that whenever I execute this, it should be able to run. Okay. Now, in order to do something like this load chunk and index, the content of the page HTML page, right, or any web page. So here, I will create a loader. And this will be equal to web based loader. And here, I'm going to give two important parameters. One is the web path. So web underscore paths, okay, is equal to, and here I will give the URL. So let me just take one URL from the GitHub IO page that is also available in the documentation of Langston. So this is what is the page over here. What I will do is that quickly, I will open a browser and show it to you how this page looks like. Okay. So if I execute this over here, you will be able to see that just a second. Okay, perfect. So here you are able to see that. Just a second to see this. Is this the same page? Yes, it is the same page. And I will be opening this. Okay, perfect. So this is the page that you will be able to see over here. And I want to probably read this entire page and use it as a rag system itself, right. So in order to read this entire page, I can just take this URL and use this web loader library. So here, if you see, I'm using this web based loader library. And I'm giving the first parameter that is the web path. Okay. The second parameter that we can specifically give is our arguments, right. So here I will rate bs dot underscore k w a r g s. Okay, so this is the second parameter, I'm just seeing the documentation. And we will give the next argument in the form of dictionaries. The first is parse underscore only. Okay. And then here, I'm basically going to use my bs for that is beautiful. So for soup trainer, okay, soup, strainer, soup strainer, okay. Yeah, soup strainer. So this is the first parameter that I will be specifically giving and understand why this is required. Because here the soup strainer, and since we are using beautiful soup over here, we really need to give the classes that it read, it needs to read from that particular page. Okay. And here, what all classes are there, let's go ahead and see this. Okay. So if I probably see over here, and if I do just inspect element, okay, so here you can see post title is there, then you have something like post content where all the content is basically available. So let us take this one as my first one. Okay, so I will just go ahead and execute this. And here, you will be able to see that this will basically be my post title. And here you have the post underscore content. And third one that I also want to take is post underscore header, because header will also have some information over here. Right. So this is done, you will be able to see that it looks absolutely fine. And here, with respect to this, we have also created this entire loader. And here, I will use a comma so that this is basically my argument over here. Right. So once we do this, then let's see whether it will execute or not. BS four is not found, as I said that BS four is not there. So let me do one thing quickly, let me open my command prompt and do the pip install requirement dot txt. Okay, so here, I will go ahead and write pip install minus our requirement dot txt. And execute it over here, you will be able to see now BS four will also get installed. And once it is getting installed, it is completed, it is good to go. Now if we go ahead and execute it, I think it should work perfect. Now loader is there. Now what we are going to do over here is that again, write loader dot load. And finally, I will be able to get in the form of text documents, text underscore documents, okay? documents. And here is my text documents. So perhaps you meant this one, let's see. So guys, there was one one error, we need to give this URL in the form of tuple. So let this get closed. And then I will write comma over here. Now I think it should be working. Let's see. Yes, perfectly. It is working right now. And if you go ahead and see your text documents, again, you will be able to get all the information from that particular web page in the form of URL, okay, or in the form of documents. So this is perfect, you're able to get every information over here. And with respect to this, you will be able to see that Yeah, you have all the information. And you can also see the page content and everything looks fine. So these are some of the ways how you can specifically make sure that you probably get all the content from a page itself. One more thing, this is not underscore, this is dash. So now I think you'll be able to see more content. Okay, so let me just open this. So here, you'll be able to see all the content itself. So this is one more way. One more way is directly to read from the PDF itself. So what I will do is that I will quickly create a PDF. Okay, so let me do one thing. And I will upload a PDF over here. Okay, so here I have created this attention dot PDF, this is nothing but attention is all you need. Now, using document loader, you can also directly read completely from the PDF itself, okay. And how to do that, see, still, we are in the data ingestion phase. And how do we do this, we'll also see that. So from the same library that we'll be using, again, we have to focus on document loader. And here, I will copy and paste it. Okay. And this is basically my PDF reader, one more library dependency will be there. Okay, so we will try to install that also. So here, I'm going to write pi PDF, or loader. Okay. And again, I will be using loader is equal to pi PDF loader, by PDF loader, and I have to give my PDF name, right. So PDF name over here, specifically is nothing but attention dot PDF. So it is in the same field, same folder itself. So here, I will write attention dot PDF. Okay, now, I think I should be getting an error. If I don't get an error, I will definitely install. Yeah, I'll install it. Now see in pip install pi PDF. So here, I will make sure that I will write requirement dot txt. And I will write pi PDF. Okay, so all these things you really need to take care of, because there will definitely be dependencies. Because and understand rag is one very important thing that really needs to get created in the form of application. So that is the reason I'm creating this completely from scratch. So pip install minus our requirement dot txt. So this is also done, I think, and pi PDF is also installed. Perfect. Then the next step, again, I will go ahead and execute this. And but here also, the same step will be required loader dot load. And this will basically be my documents. And
I will execute this. Once I probably execute this, this is my docs. Okay, perfect. So here you'll be able to see my entire PDF has been read and it is in front of you. Now the next step now see this load data source part is basically done, we know how to load it. Okay, now we will move towards this transform. Okay, then we will go ahead towards embed. Okay, so the first part of load I've actually done with respect to PDF web based with respect to text file. If you go ahead and check out the language and documentation this they are still more amazing ways to do for Excel file for readme file, different directories even for directives, let's say that you have a lot of files in the directory itself many files and you can also load that. So we'll try to see in upcoming videos more about different different examples. But here just to give an idea about load data source and load is actually done. Now let's go ahead and do this transform and not transform is very much important. Now understand guys, this is your entire PDF documents, right? Now you need to convert this PDF documents into chunks. Now how do you do that? And again, they are multiple ways. And this way is entirely falls inside the category of text splitter inside chain. So here we are going to write from line chain from line chain dot text splitter import recursive recursive character text splitter will try to split it with the with the help of text itself. So here I'm going to create my text splitter is equal to recursive character text splitter. And here I'm going to specifically use my chunk underscore size is equal to 1000. I want this specific size to be the chunk. And let's say the overlap chunk, chunk overlap, okay, chunk overlap. So here I will go ahead and write chunk underscore overlap is equal to 200. I'll keep this as 200. Okay. So once I do this, now this text splitter will be responsible text underscore splitter dot split documents will be responsible in splitting all the specific documents that I give over here. Okay, so this docs will be there. And this will be my documents final documents that I will be able to see. Okay, let's display this documents, the top five documents still here. Okay. And let's see this. So here you will be able to see provided proper attrition is Googled by grant permission, best model, this, this, this, this, everything is there, attention dot PDF, right. So all the information is basically there. And with respect to this, we really want to see the more things more are different, different, all the documents in short. So you can also go ahead and see the entire documents over here. Okay, done. So these are all my documents over here, transformers, everything is available over here. Now see, this is my entire documents, it has been divided into proper smaller chunks. Now we can take this chunks and we can convert that into vectors. Okay. And that is what is given you an idea over here, right? Like how we have transformed, we have basically taken this entire PDF document, and we have divided that into chunks. Now it's a time that we understand how to probably convert this into vectors. And for this, we will also be using some different different vector vector embedding techniques, right? One of the vector embedding techniques that I will be showing you is with respect to open AI. So here, now, we will go ahead and probably write about vector embeddings and vector store, right? vector embeddings is a technique wherein we convert a text into vectors. Okay, so how do we do that again, with the help of open AI. So here, I'm going to write from Langston underscore community dot embeddings, import, open AI, open AI embeddings, you can also use olama embeddings, it is up to you since if you don't have open API, you can use olama embeddings directly. But the performance of open AI embeddings is better, far more better than olama embeddings. Okay. So here, what I'm actually going to do next thing is that first of all, we need to understand how to probably create vectors. Again, to create vectors, we can use open AI embeddings. But after creating the vectors, we also need to store in the vector store. That is what it is, right? This vector store is like a kind of a database. embeddings can be embeddings, the technique where you convert text into vectors, but later those text needs to be stored in some kind of vector store, right? So for this reason, we will be using something called as chroma DB. So they are come, they are a couple of, there are a couple of vector databases that has been provided by Langston itself. One is chroma, one is fires. As we go ahead, you know how to create this kind of vector database in the cloud also, I will show it to you. So from Langston underscore community, I will be using dot vector stores. And here I'm going to import one is nothing but chroma. So with respect to this, we will go ahead and create my DB. And here I'm going to write chroma dot from underscore documents, okay, from documents. And here, I'm going to basically give my entire document, okay, entire documents, and I'll not give the entire documents, let's just give the first 20 documents itself, because it will take more time to create the embeddings, right. And here, the embeddings that I'm going to specifically use is nothing but opening embeddings. Once I execute this, I think I'm going to get an error saying that chroma is not available. I don't know whether I've installed chroma or not. So here you can see pip install chroma DB, I rarely require chroma DB. So again, I'll go back to my requirement or txt. The reason why I'm showing you all this completely from scratch so that you whatever error you face, you should be able to fix it up. Okay, so I'm going to delete this and let's go ahead and write pip install. One more library that I will be using is nothing but fires CPU, okay, fire CPU, because fires is also one type of chroma one type of vector database. Okay, so minus our requirement or txt. So let's install both the specific libraries, it will take some amount of time. Again, it depends on your internet speed and how fast your system is. But yes, I think five CPU and chroma DB is the kind of vector databases that we will be using. One assignment I'll give you try to also use Lance vector database from again seeing the Langston documentation, you can actually do it. Okay. Now this is done once this will probably be done, then we can go ahead and check by executing it whether it is working now or not. But at the end of the day here, what we are actually doing, we have imported open AI embeddings, the vector database that we are going to use is chroma, then chroma dot from document, I'm giving the entire document and using this embeddings, it will be storing inside this particular vector store. Now this DB, we can store it in our local, we can store it in the cloud wherever you specifically want. Okay, so guys, the installation has been completed. Now let's go ahead and execute this chroma part. Now here, you'll be able to see that entire embedding will basically happen. And again, we are going to use the open AI embedding for this. And now this DB is nothing but our vector database. Okay, so if I probably consider vector database, and now all I have to do is that I can query anything with respect to this particular vector database to retrieve any kind of result that I really want. So I will create a query First of all, so here, let me go ahead and write my query. So query is like, I will go ahead and write, who are the authors? Who are the authors of attention is all you need to research paper. Okay, so this is my query that I'm going to specifically ask, let's see whether it will be able to understand this or and it will be able to give us the result. So here, I will go ahead and write DB dot similarity search. Now there are multiple option, similarity search similarity search by vector, if you want, you can also convert your data into vectors by using again, the open AI embeddings, and you can query it from here. But we are just going to use the similarity search since we are just going to use the query over here, right. So if I write DB dot similarity search of query, so this will basically be my result. Okay, now let's go ahead and see my result. Okay, and let's execute this. So here, you will be able to see that I'm able to see multiple information, like four documents has been over here. So let's take the first document. And let's see how the result is. So here, I'm going to basically take zero. And inside that the field name is nothing but page content. So I'm going to basically write my page content. Now here, you'll be able to see prop provided proper attribution is provided Google here by grant permission. And here, you'll be able to see Google brain, all the email IDs, all the researchers names are actually available. So that basically means it is able to research, it is able to retrieve the results of the all the scientists who are involved, all the researchers who are involved in creating this particular paper, I can also do one more thing, I can go ahead and write what is attention is all you need, okay, attention is all you need. Okay, so let's see what kind of results I will be able to get and understand this is entirely a rack pipeline, which is being able to come from the entire documentation. And here, you'll be able to see that result is also coming. Let me do write something which is available in the research paper. So let me open my research paper over here. And let me ask some question directly. Okay, from here, you will be able to see, see when I'm searching attention is all you need. It is coming from here itself, right. So let me use something over here. Let me just write over here encoder is what is an
attention function, let me just go and search this. Okay, let's see. Attention is something something some text is there we have. So if I go ahead and execute it, and here will be instead of performing a single attention function with the model dimension values, keys, and all the information is basically coming up, right. And here is a very good result. And this is directly coming from the vector database. That is the most amazing thing over here, right? Now, you may be also thinking, Trish, can we also use the fires database, fire depicted database that we have used, right. So let me just show you with respect to that also fires. And here, I'm going to use the fire vector database. And this will also give you an idea how you can actually store that embeddings into fires database itself. And we'll also do that. So here, I'm going to write from Lang chain, Lang chain, underscore, community, dot vector stores, I'm going to import my FA is fires. And then I'm going to also use TB is equal to FA is dot from documents, the same thing, right. So from underscore documents, and here, I'm going to use my documents, okay. And let's say I go ahead and do the embedding for the first 20 documents. And then here, also, I'm going to use my open AI embeddings. And I think this is also my another DB. So I will write it at DB one, which is my files database, okay, from documents, okay, no worries. So here, the spelling mistake was there. And I think this will also work. Now DB one is also ready. And what I can do, I can use the same thing and paste it over here, okay. And just write DB one dot similarity search. And here you can see instead of performing a single attention, everything is probably coming up. So I've just shown you the example of both fires and chroma database chroma vector database. As I said, one assignment will be given to you as regarding Lance vector database, you can go and search it for that in the in the in the language and documentation, we are going to continue the language series. And now we will be developing advanced rack pipeline using retriever and chain concepts that are available in language. Now, before I go ahead, I really want to talk about a funny incident that really happened recently, just today itself. So what I did, what I do is that every day morning, I usually go and play some badminton, you know, I play for one hour, one and a half hour. And usually a lot of my friends and neighbors usually come and play. And so today, what happened after playing I went today in this look, okay, I played around three to four games. And then one of my neighbors said, Hey, Chris, you're Chris, right? You just got identified right now, just by seeing your shoes, I identified you, but your look has completely changed. So let me know if this is true in the comment section of this particular video. But I am liking this look. You know, it's like, so much of less maintenance, you don't have to maintain your beard, your moustache or your hair also, right? It looks super cool. Now, let's go ahead and work towards this specific project. In our previous tutorial, what we had actually done is that we had created this simple rack pipeline, okay, we had a data data source, like we took PDF, we took website, then we load that particular data set using different, different data ingestion techniques that are available in language. Then we did transformation, wherein we broke down our bigger PDFs into chunks. And then we converted all these particular chunks into vectors and stored it in a vector store. And then with the help of query, we are able to probably retrieve some of the data that is available in the vector store. Now this is one step. Now the further step after this particular query vector now understand query vector are not that efficient, you know, when in terms of retrieving the entire results. Here, we will also specifically use LLM models. Okay, so now what we will try to do is that using some prompts, okay, and we will take this specific prompts, we will take this particular data using the concept of chain and retriever. Okay, and understand this topic is very important, because this is where your advanced rack pipeline implementation will start using chain and retriever, we will also use and in this chain and retriever, what we do is that we specifically use LLM models, it can be open source, it can be paid, whatever model you want. So we will specifically use this LLM model. And based on this prompt, we will try to get the response on what we are specifically looking, right. So there will be a lot of customization that will be added once we implement this specific part. In the first part, we discussed about this. And this is the second part that we are going to discuss, okay, how we can use chain how we can use a retriever, what exactly is chain how we can integrate in this particular LLM model, there is a concept of something called a stuff document chain, what exactly it is. So we will discuss everything all about it. And here, we are also going to do a practical implementation. So please make sure that you watch this video till the end. And we are going to learn a lot of things. Okay, so here, the first step what we had done on already, we have implemented this in our previous tutorial also. So here, you'll be able to see that I am trying to read attention dot PDF, which is present in a folder. And then we just write loader dot load and we get the documents. Okay, so these are all the documents that will be available in this specific PDF. Okay, then what we are specifically doing next step is that from Langston dot text splitter, we will be using recursive character text splitter, wherein we convert the entire document into chunks, right, and then we are probably using this text splitter. And we are using a chunk size of 1000 overlap of 20. So this everything is implemented in my previous videos, right. So we are going to split this entire document and save it in this particular documents. In the previous tutorial, we have implemented all these things. Now, here, what we are going to take do now we'll take all these documents, and then we will convert it into a vector store, right. So vector store for that we are using this files. Okay, so here we are going we can use olama embedding or open AI embedding, right? As I said, you open AI embedding is very much advanced, and it will perform better than olama embedding. If you don't have open AI API key use olama embedding, instead of open AI embedding over here, you can just write olama embedding, right. So from here, I will be using fires, which is again a kind of vector store. And it has been developed by meta. So fires dot from documents document of 20. So I'm just taking the first 20 documents, and I'm just writing open AI embedding, let's make it to 30. So that it will have some amount of data. Right now we are specifically using open AI embedding and this DB that you specifically see is a my vector store. Okay, so here you can see vector store fires fires of type. Okay, perfect. Now, any question that I ask attention function can be described as a mapping query. And then we can take this vector store and just write dot similarity search on this query. And we will get the result over here. Okay, so this all things we have actually done in our previous video. Now is the most important thing how I can combine prompt along with chains and retriever and then probably get a response based on the prompt. Okay. So since many people have the use of only open source, they have the access of open source model LLM model. So I'm going to use olama from Langston dot community dot LLM import olama. Then I'm going to use olama over here model will be lama too. If you don't have lama to just to go to command prompt after downloading olama, I hope you everybody knows how to download it if you're seeing my series of videos. Here, you can just write olama run lama too. Right. So once you write like this, then the model will get downloaded, right? If it is already downloaded, it will be coming something like this. Okay. So this is the first step that you really need to do then have written or from Langston dot community dot LLM olama load olama. So whatever olama model we are specifically using that is lama too. So this is my open source model. So if you see lama LLM, it is nothing but olama. Now is the time we will start designing a prompt template. Now in order to design the chat prompt template, I will be using chat Langston underscore core dot prompts import chat prompt template. Okay. And then from chat prompt template from template, I'm just writing like this. So I'm saying answer the following question based only on the provided context. Now see, we are trying to develop a q&a q&a chat bot based on the context, it should provide me the response. Previously, what we are doing using vector store, we used to, if you see the code over here, we used to query the vector score, right vector store by using similarity search algorithm. But here what we are doing here, we are defining our own prompt. And we are saying, Hey, answer the following question based on the provided context, right? I will, I'm simply I'm writing away, I'll tip you $1,000. If you find the answer helpful, okay, if the user find the answer helpful, okay, just at least by seeing money, the AI may perform well. And then we are giving our context. And then question will be input. Okay. How, why I'm writing in this specific way, because this chain and retriever, right, you'll be understanding this context will be autofilled. And this input will also get autofilled. Okay, how it will get autofilled, I'll let you know. So now what I will do, I will execute this. Now I will go ahead and implement about chain. It is always a good idea, okay, to probably go to your browser and check about each and every topic that I'm explaining. So what does chain refer to chain referred to a sequence of calls, whether to an LLM or tool or data pre processing step. The primary supported way to do this is LCL. Okay. Now, if you talk about chain over here, there are multiple functions with respect to chain.
